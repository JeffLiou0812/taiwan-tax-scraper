#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
è²¡æ”¿éƒ¨æ³•è¦è‰æ¡ˆçˆ¬èŸ² - éŒ¯èª¤é˜²è­·åŠ å¼·ç‰ˆ
åŸºæ–¼éå»éŒ¯èª¤ç¶“é©—å„ªåŒ–çš„ç‰ˆæœ¬

ç›®æ¨™ç¶²ç«™: https://law-out.mof.gov.tw/DraftForum.aspx
ç‰ˆæœ¬: 3.0 Error-Protected
å»ºç«‹æ—¥æœŸ: 2025-08-20
"""

import requests
import json
import pandas as pd
from datetime import datetime, timezone, timedelta
import re
from pathlib import Path
import hashlib
import time
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse

class DraftLawScraperProtected:
    """éŒ¯èª¤é˜²è­·åŠ å¼·ç‰ˆæ³•è¦è‰æ¡ˆçˆ¬èŸ²"""
    
    def __init__(self, data_dir="data"):
        """åˆå§‹åŒ–çˆ¬èŸ²"""
        self.base_url = "https://law-out.mof.gov.tw/DraftForum.aspx"
        self.data_dir = Path(data_dir)
        self.data_dir.mkdir(exist_ok=True)
        
        # éŒ¯èª¤é˜²è­·1: å®Œæ•´çš„è«‹æ±‚æ¨™é ­ï¼ˆé¿å…robots.txté˜»æ“‹ï¼‰
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'zh-TW,zh;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
            'Cache-Control': 'max-age=0'
        }
        
        self.tz_taipei = timezone(timedelta(hours=8))
        self.session = requests.Session()
        self.session.headers.update(self.headers)
        
    def validate_url(self, url):
        """
        éŒ¯èª¤é˜²è­·2: URLé©—è­‰å’Œä¿®å¾©
        å­¸ç¿’è‡ªéå»çš„ law.dot.gov.twhome.jsp éŒ¯èª¤
        """
        if not url:
            return None
            
        url = url.strip()
        
        # ä¿®å¾©å¸¸è¦‹URLå•é¡Œ
        if not url.startswith(('http://', 'https://')):
            # åˆ¤æ–·æ˜¯å¦ç‚ºç›¸å°è·¯å¾‘
            if url.startswith('/'):
                url = 'https://law-out.mof.gov.tw' + url
            else:
                url = 'https://' + url
        
        # é©—è­‰URLæ ¼å¼
        try:
            result = urlparse(url)
            if all([result.scheme, result.netloc]):
                return url
        except:
            pass
            
        return None
    
    def convert_roc_date_safe(self, roc_date_str):
        """
        éŒ¯èª¤é˜²è­·3: ç©©å¥çš„æ—¥æœŸè½‰æ›
        æ”¯æ´å¤šç¨®æ°‘åœ‹å¹´æ ¼å¼
        """
        if not roc_date_str:
            return None, roc_date_str
            
        roc_date_str = str(roc_date_str).strip()
        
        # å¤šé‡æ¨¡å¼åŒ¹é…ï¼ˆå­¸ç¿’è‡ªéå»éŒ¯èª¤ï¼‰
        patterns = [
            (r'(\d{2,3})å¹´(\d{1,2})æœˆ(\d{1,2})æ—¥', '%Y-%m-%d'),
            (r'(\d{2,3})\.(\d{1,2})\.(\d{1,2})', '%Y-%m-%d'),
            (r'(\d{2,3})/(\d{1,2})/(\d{1,2})', '%Y-%m-%d'),
            (r'æ°‘åœ‹(\d{2,3})å¹´(\d{1,2})æœˆ(\d{1,2})æ—¥', '%Y-%m-%d')
        ]
        
        for pattern, date_format in patterns:
            match = re.search(pattern, roc_date_str)
            if match:
                try:
                    roc_year = int(match.group(1))
                    month = int(match.group(2))
                    day = int(match.group(3))
                    
                    # è½‰æ›ç‚ºè¥¿å…ƒå¹´
                    ad_year = roc_year + 1911
                    iso_date = f"{ad_year:04d}-{month:02d}-{day:02d}"
                    
                    return iso_date, roc_date_str  # åŒæ™‚è¿”å›å…©ç¨®æ ¼å¼
                except:
                    continue
        
        return None, roc_date_str
    
    def fetch_draft_laws(self):
        """
        çˆ¬å–æ³•è¦è‰æ¡ˆåˆ—è¡¨
        åŒ…å«å®Œæ•´éŒ¯èª¤è™•ç†
        """
        all_drafts = []
        
        try:
            print("ğŸ” é–‹å§‹çˆ¬å–æ³•è¦è‰æ¡ˆ...")
            
            # éŒ¯èª¤é˜²è­·4: é‡è©¦æ©Ÿåˆ¶
            max_retries = 3
            for attempt in range(max_retries):
                try:
                    response = self.session.get(
                        self.base_url,
                        timeout=30,
                        verify=True
                    )
                    
                    if response.status_code == 200:
                        break
                    else:
                        print(f"âš ï¸ å˜—è©¦ {attempt + 1}/{max_retries}: ç‹€æ…‹ç¢¼ {response.status_code}")
                        time.sleep(2)
                        
                except requests.exceptions.RequestException as e:
                    print(f"âš ï¸ ç¶²è·¯éŒ¯èª¤ (å˜—è©¦ {attempt + 1}/{max_retries}): {str(e)}")
                    if attempt < max_retries - 1:
                        time.sleep(3)
                    else:
                        raise
            
            # éŒ¯èª¤é˜²è­·5: è§£æHTML
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # å°‹æ‰¾è³‡æ–™è¡¨æ ¼ï¼ˆæ ¹æ“šå¯¦éš›ç¶²ç«™çµæ§‹èª¿æ•´ï¼‰
            table = soup.find('table', {'class': 'table'}) or soup.find('table')
            
            if not table:
                print("âš ï¸ æœªæ‰¾åˆ°è³‡æ–™è¡¨æ ¼ï¼Œå˜—è©¦å…¶ä»–è§£ææ–¹å¼...")
                # å‚™ç”¨è§£ææ–¹å¼
                rows = soup.find_all('tr')
            else:
                rows = table.find_all('tr')[1:]  # è·³éæ¨™é¡Œåˆ—
            
            for row in rows:
                try:
                    cols = row.find_all('td')
                    
                    if len(cols) >= 3:  # ç¢ºä¿æœ‰è¶³å¤ çš„æ¬„ä½
                        # æå–è³‡æ–™
                        date_text = cols[0].get_text(strip=True)
                        title_element = cols[1].find('a')
                        end_date_text = cols[2].get_text(strip=True) if len(cols) > 2 else ""
                        
                        # è™•ç†æ¨™é¡Œå’Œé€£çµ
                        if title_element:
                            title = title_element.get_text(strip=True)
                            raw_url = title_element.get('href', '')
                            
                            # éŒ¯èª¤é˜²è­·6: URLè™•ç†
                            if raw_url:
                                # å®Œæ•´URLè™•ç†
                                if not raw_url.startswith('http'):
                                    raw_url = urljoin(self.base_url, raw_url)
                                
                                url = self.validate_url(raw_url)
                            else:
                                url = None
                        else:
                            title = cols[1].get_text(strip=True)
                            url = None
                        
                        # æ—¥æœŸè½‰æ›
                        iso_date, roc_date = self.convert_roc_date_safe(date_text)
                        end_iso_date, end_roc_date = self.convert_roc_date_safe(end_date_text)
                        
                        # å»ºç«‹è‰æ¡ˆè³‡æ–™
                        draft = {
                            'title': title,
                            'announcement_date': iso_date,
                            'announcement_date_roc': roc_date,
                            'end_date': end_iso_date,
                            'end_date_roc': end_roc_date,
                            'url': url,
                            'original_url': raw_url if title_element else None,
                            'status': self.check_status(end_iso_date),
                            'source': 'MOF_Taiwan_Draft',
                            'scrape_time': datetime.now(self.tz_taipei).isoformat()
                        }
                        
                        # ç”Ÿæˆå”¯ä¸€ID
                        draft['id'] = self.generate_unique_id(draft)
                        
                        all_drafts.append(draft)
                        
                except Exception as e:
                    print(f"âš ï¸ è§£æå–®ç­†è³‡æ–™éŒ¯èª¤: {str(e)}")
                    continue
            
            print(f"âœ… æˆåŠŸçˆ¬å– {len(all_drafts)} ç­†æ³•è¦è‰æ¡ˆ")
            
        except Exception as e:
            print(f"âŒ çˆ¬å–å¤±æ•—: {str(e)}")
            import traceback
            traceback.print_exc()
        
        return all_drafts
    
    def check_status(self, end_date_str):
        """æª¢æŸ¥è‰æ¡ˆç‹€æ…‹"""
        if not end_date_str:
            return "é€²è¡Œä¸­"
        
        try:
            end_date = datetime.strptime(end_date_str, '%Y-%m-%d')
            today = datetime.now()
            
            if end_date < today:
                return "å·²çµæŸ"
            else:
                return "é€²è¡Œä¸­"
        except:
            return "æœªçŸ¥"
    
    def generate_unique_id(self, draft):
        """ç”Ÿæˆå”¯ä¸€è­˜åˆ¥ç¢¼"""
        # ä½¿ç”¨æ¨™é¡Œå’Œæ—¥æœŸç”ŸæˆID
        content = f"{draft.get('title', '')}{draft.get('announcement_date', '')}"
        return hashlib.md5(content.encode('utf-8')).hexdigest()[:12]
    
    def compare_and_update(self, new_drafts):
        """
        æ¯”å°æ­·å²è¨˜éŒ„ï¼Œæ‰¾å‡ºæ–°å¢çš„è‰æ¡ˆ
        """
        history_file = self.data_dir / "draft_history.json"
        
        # è®€å–æ­·å²è¨˜éŒ„
        if history_file.exists():
            try:
                with open(history_file, 'r', encoding='utf-8') as f:
                    history = json.load(f)
            except:
                history = []
        else:
            history = []
        
        # å»ºç«‹IDé›†åˆé€²è¡Œæ¯”å°
        history_ids = {item['id'] for item in history if 'id' in item}
        
        # æ‰¾å‡ºæ–°è‰æ¡ˆ
        new_items = []
        for draft in new_drafts:
            if draft['id'] not in history_ids:
                new_items.append(draft)
        
        # æ›´æ–°æ­·å²è¨˜éŒ„
        if new_items:
            history.extend(new_items)
            with open(history_file, 'w', encoding='utf-8') as f:
                json.dump(history, f, ensure_ascii=False, indent=2)
        
        return new_items, history
    
    def save_results(self, drafts, filename_prefix="drafts"):
        """
        å„²å­˜çµæœç‚ºJSONå’ŒCSVæ ¼å¼
        """
        if not drafts:
            print("âš ï¸ æ²’æœ‰è³‡æ–™å¯å„²å­˜")
            return None
        
        timestamp = datetime.now(self.tz_taipei).strftime('%Y%m%d_%H%M%S')
        
        # å„²å­˜JSON
        json_file = self.data_dir / f'{filename_prefix}_{timestamp}.json'
        with open(json_file, 'w', encoding='utf-8') as f:
            json.dump(drafts, f, ensure_ascii=False, indent=2)
        
        # å„²å­˜CSV
        csv_file = self.data_dir / f'{filename_prefix}_{timestamp}.csv'
        df = pd.DataFrame(drafts)
        df.to_csv(csv_file, index=False, encoding='utf-8-sig')
        
        print(f"ğŸ’¾ è³‡æ–™å·²å„²å­˜:")
        print(f"   JSON: {json_file}")
        print(f"   CSV: {csv_file}")
        
        return json_file
    
    def generate_report(self, new_drafts, total_drafts):
        """ç”ŸæˆåŸ·è¡Œå ±å‘Š"""
        report = {
            'execution_time': datetime.now(self.tz_taipei).isoformat(),
            'total_drafts': len(total_drafts),
            'new_drafts': len(new_drafts),
            'has_new': len(new_drafts) > 0,
            'status_summary': {},
            'error_fixes_applied': [
                'URL validation and correction',
                'Multiple date format support',
                'Retry mechanism for network errors',
                'Comprehensive error handling'
            ]
        }
        
        # çµ±è¨ˆç‹€æ…‹
        if total_drafts:
            df = pd.DataFrame(total_drafts)
            if 'status' in df.columns:
                report['status_summary'] = df['status'].value_counts().to_dict()
        
        # å„²å­˜å ±å‘Š
        report_file = self.data_dir / "draft_report.json"
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)
        
        return report

def main():
    """ä¸»ç¨‹å¼ - åŒ…å«å®Œæ•´éŒ¯èª¤è™•ç†"""
    print("="*60)
    print("ğŸ›ï¸ è²¡æ”¿éƒ¨æ³•è¦è‰æ¡ˆçˆ¬èŸ² - éŒ¯èª¤é˜²è­·åŠ å¼·ç‰ˆ")
    print(f"ğŸ• åŸ·è¡Œæ™‚é–“: {datetime.now()}")
    print("ğŸ›¡ï¸ å·²æ‡‰ç”¨éå»7å¤©çš„æ‰€æœ‰éŒ¯èª¤ä¿®å¾©")
    print("="*60)
    
    try:
        scraper = DraftLawScraperProtected()
        
        # çˆ¬å–è³‡æ–™
        print("\nğŸ“¡ é–‹å§‹çˆ¬å–æ³•è¦è‰æ¡ˆ...")
        drafts = scraper.fetch_draft_laws()
        
        if not drafts:
            print("âš ï¸ æœªç²å–ä»»ä½•è‰æ¡ˆè³‡æ–™")
            return
        
        # æ¯”å°æ­·å²
        print("\nğŸ“Š æ¯”å°æ­·å²è³‡æ–™...")
        new_items, all_drafts = scraper.compare_and_update(drafts)
        
        if new_items:
            print(f"ğŸ†• ç™¼ç¾ {len(new_items)} ç­†æ–°è‰æ¡ˆ!")
            for i, item in enumerate(new_items[:5], 1):  # é¡¯ç¤ºå‰5ç­†
                print(f"   {i}. {item.get('title', 'ç„¡æ¨™é¡Œ')[:50]}...")
        else:
            print("âœ¨ æ²’æœ‰æ–°çš„æ³•è¦è‰æ¡ˆ")
        
        # å„²å­˜çµæœ
        print("\nğŸ’¾ å„²å­˜è³‡æ–™...")
        scraper.save_results(drafts)
        
        # ç”Ÿæˆå ±å‘Š
        print("\nğŸ“‹ ç”Ÿæˆå ±å‘Š...")
        report = scraper.generate_report(new_items, all_drafts)
        
        print(f"\nğŸ“Š åŸ·è¡Œçµ±è¨ˆ:")
        print(f"   â€¢ ç¸½è‰æ¡ˆæ•¸: {report['total_drafts']}")
        print(f"   â€¢ æ–°å¢è‰æ¡ˆ: {report['new_drafts']}")
        print(f"   â€¢ ç‹€æ…‹åˆ†å¸ƒ: {report.get('status_summary', {})}")
        
        print("\nâœ… æ³•è¦è‰æ¡ˆçˆ¬èŸ²åŸ·è¡Œå®Œæˆï¼")
        print("ğŸ›¡ï¸ æ‰€æœ‰éŒ¯èª¤é˜²è­·æ©Ÿåˆ¶å‡å·²ç”Ÿæ•ˆ")
        
    except Exception as e:
        print(f"\nâŒ åŸ·è¡Œå¤±æ•—: {str(e)}")
        import traceback
        traceback.print_exc()
        exit(1)

if __name__ == "__main__":
    main()
