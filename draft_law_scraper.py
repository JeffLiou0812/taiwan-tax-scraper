#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ”¹é€²ç‰ˆè²¡æ”¿éƒ¨æ³•è¦è‰æ¡ˆçˆ¬èŸ²
- æ›´æ™ºèƒ½çš„é‡è¤‡æª¢æ¸¬æ©Ÿåˆ¶
- éæœŸè‰æ¡ˆç‹€æ…‹è¿½è¹¤
- URL-based å»é‡é‚è¼¯
- å®Œæ•´çš„è®ŠåŒ–è¿½è¹¤ (æ–°å¢/æ›´æ–°/éæœŸ)

ç›®æ¨™ç¶²ç«™: https://law-out.mof.gov.tw/DraftForum.aspx
ä½œè€…: è‡ªå‹•åŒ–ç¨…å‹™æ³•è¦è¿½è¹¤ç³»çµ±
ç‰ˆæœ¬: 2.0 Enhanced
æ—¥æœŸ: 2025-08-19
"""

import requests
import json
import pandas as pd
from datetime import datetime, timezone, timedelta
import re
from pathlib import Path
import hashlib
import time

class ImprovedDraftLawScraper:
    """æ”¹é€²ç‰ˆæ³•è¦è‰æ¡ˆçˆ¬èŸ²é¡åˆ¥"""
    
    def __init__(self, data_dir="data"):
        """
        åˆå§‹åŒ–çˆ¬èŸ²
        
        Args:
            data_dir (str): è³‡æ–™å„²å­˜ç›®éŒ„
        """
        self.base_url = "https://law-out.mof.gov.tw/DraftForum.aspx"
        self.data_dir = Path(data_dir)
        self.data_dir.mkdir(exist_ok=True)
        
        # è¨­å®šè«‹æ±‚æ¨™é ­ - æ¨¡æ“¬æ­£å¸¸ç€è¦½å™¨
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'zh-TW,zh;q=0.8,en-US;q=0.5,en;q=0.3',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
        }
        
        # å°ç£æ™‚å€
        self.tz_taipei = timezone(timedelta(hours=8))
        
    def generate_unique_id(self, draft):
        """
        ç”Ÿæˆå”¯ä¸€è­˜åˆ¥ç¢¼
        
        Args:
            draft (dict): æ³•è¦è‰æ¡ˆè³‡æ–™
            
        Returns:
            str: 12ä½å”¯ä¸€è­˜åˆ¥ç¢¼
        """
        # ä½¿ç”¨ URL ä½œç‚ºä¸»è¦è­˜åˆ¥ç¢¼ï¼Œå¦‚æœæ²’æœ‰ URL å‰‡ä½¿ç”¨æ¨™é¡Œ+æ—¥æœŸ
        if draft.get('url'):
            return hashlib.md5(draft['url'].encode('utf-8')).hexdigest()[:12]
        else:
            content = f"{draft.get('title', '')}{draft.get('date', '')}"
            return hashlib.md5(content.encode('utf-8')).hexdigest()[:12]
    
    def convert_roc_date(self, roc_date):
        """
        è½‰æ›æ°‘åœ‹å¹´ç‚ºè¥¿å…ƒå¹´
        
        Args:
            roc_date (str): æ°‘åœ‹å¹´æ—¥æœŸ (æ ¼å¼: 114.08.19)
            
        Returns:
            str: è¥¿å…ƒå¹´æ—¥æœŸ (æ ¼å¼: 2025-08-19)
        """
        try:
            parts = roc_date.split('.')
            if len(parts) == 3:
                year = int(parts[0]) + 1911  # æ°‘åœ‹å¹´è½‰è¥¿å…ƒå¹´
                month = int(parts[1])
                day = int(parts[2])
                return f"{year}-{month:02d}-{day:02d}"
        except (ValueError, IndexError):
            print(f"âš ï¸ æ—¥æœŸæ ¼å¼éŒ¯èª¤: {roc_date}")
        
        return roc_date
    
    def fetch_draft_laws(self):
        """
        çˆ¬å–æ³•è¦è‰æ¡ˆ
        
        Returns:
            list: æ³•è¦è‰æ¡ˆæ¸…å–®
        """
        print(f"ğŸ” é–‹å§‹çˆ¬å–æ³•è¦è‰æ¡ˆ...")
        print(f"ğŸ“¡ ç›®æ¨™ç¶²ç«™: {self.base_url}")
        
        try:
            # ç™¼é€è«‹æ±‚
            print("ğŸ“¡ ç™¼é€ç¶²è·¯è«‹æ±‚...")
            response = requests.get(self.base_url, headers=self.headers, timeout=30)
            response.raise_for_status()
            
            print(f"âœ… ç¶²ç«™å›æ‡‰æˆåŠŸ (ç‹€æ…‹ç¢¼: {response.status_code})")
            print(f"ğŸ“„ å›æ‡‰å…§å®¹å¤§å°: {len(response.text)} å­—å…ƒ")
            
            content = response.text
            
            # ä½¿ç”¨æ­£å‰‡è¡¨é”å¼è§£æè¡¨æ ¼çµæ§‹
            # æ¨¡å¼ï¼šåºè™Ÿ | æ—¥æœŸ | æ¨™é¡Œé€£çµ | é å‘Šçµ‚æ­¢æ—¥
            print("ğŸ” è§£æç¶²é å…§å®¹...")
            pattern = r'(\d+)\.\s*\|\s*(\d{3}\.\d{2}\.\d{2})\s*\|\s*\[([^\]]+)\]\(([^)]+)\)\s*ï¼ˆé å‘Šçµ‚æ­¢æ—¥(\d{3}\.\d{2}\.\d{2})ï¼‰'
            
            matches = re.findall(pattern, content)
            
            if not matches:
                print("âš ï¸ ä¸»è¦è§£ææ–¹æ³•æœªæ‰¾åˆ°è³‡æ–™ï¼Œå˜—è©¦å‚™ç”¨è§£æ...")
                return self._alternative_parsing(content)
            
            print(f"âœ… æ­£å‰‡è¡¨é”å¼æ‰¾åˆ° {len(matches)} ç­†åŒ¹é…")
            
            # è½‰æ›ç‚ºçµæ§‹åŒ–è³‡æ–™
            draft_laws = []
            current_time = datetime.now(self.tz_taipei).isoformat()
            
            for i, match in enumerate(matches, 1):
                seq_num, date_str, title, url, end_date = match
                
                print(f"ğŸ“ è™•ç†ç¬¬ {i} ç­†: {title[:30]}...")
                
                draft_law = {
                    'sequence': int(seq_num),
                    'date': self.convert_roc_date(date_str),
                    'roc_date': date_str,
                    'title': title.strip(),
                    'url': url.strip(),
                    'end_date': self.convert_roc_date(end_date),
                    'roc_end_date': end_date,
                    'scraped_at': current_time,
                    'source': 'MOF_DraftForum',
                    'status': 'active',  # æ–°å¢ç‹€æ…‹æ¬„ä½
                    'scraper_version': '2.0_enhanced'
                }
                
                # ç”Ÿæˆå”¯ä¸€ID
                draft_law['unique_id'] = self.generate_unique_id(draft_law)
                
                draft_laws.append(draft_law)
            
            print(f"âœ… æˆåŠŸè§£æ {len(draft_laws)} ç­†æ³•è¦è‰æ¡ˆ")
            return draft_laws
            
        except requests.exceptions.RequestException as e:
            print(f"âŒ ç¶²è·¯è«‹æ±‚å¤±æ•—: {e}")
            print("ğŸ”„ å¯èƒ½çš„åŸå› :")
            print("   â€¢ ç¶²è·¯é€£ç·šå•é¡Œ")
            print("   â€¢ ç›®æ¨™ç¶²ç«™æš«æ™‚ç„¡æ³•å­˜å–")
            print("   â€¢ è«‹æ±‚è¢«é˜»æ“‹")
            return []
        except Exception as e:
            print(f"âŒ è§£æéç¨‹ç™¼ç”ŸéŒ¯èª¤: {e}")
            print("ğŸ”„ å¯èƒ½çš„åŸå› :")
            print("   â€¢ ç¶²ç«™çµæ§‹æ”¹è®Š")
            print("   â€¢ æ­£å‰‡è¡¨é”å¼éœ€è¦èª¿æ•´")
            return []
    
    def _alternative_parsing(self, content):
        """
        å‚™ç”¨è§£ææ–¹æ³•
        
        Args:
            content (str): ç¶²é å…§å®¹
            
        Returns:
            list: æ³•è¦è‰æ¡ˆæ¸…å–®
        """
        print("ğŸ”„ ä½¿ç”¨å‚™ç”¨è§£ææ–¹æ³•...")
        
        draft_laws = []
        lines = content.split('\n')
        current_time = datetime.now(self.tz_taipei).isoformat()
        
        sequence = 1
        for line in lines:
            line = line.strip()
            
            # å°‹æ‰¾åŒ…å«æ—¥æœŸå’Œé€£çµçš„è¡Œ
            if '114.' in line and 'join.gov.tw' in line:
                print(f"ğŸ“ å‚™ç”¨æ–¹æ³•æ‰¾åˆ°: {line[:50]}...")
                
                # å˜—è©¦æå–åŸºæœ¬è³‡è¨Š
                draft_law = {
                    'sequence': sequence,
                    'title': line.strip(),
                    'scraped_at': current_time,
                    'source': 'MOF_DraftForum_Alternative',
                    'status': 'active',
                    'scraper_version': '2.0_alternative'
                }
                
                # å˜—è©¦æå–URL
                url_match = re.search(r'https://[^\s)]+', line)
                if url_match:
                    draft_law['url'] = url_match.group()
                
                # ç”Ÿæˆå”¯ä¸€ID
                draft_law['unique_id'] = self.generate_unique_id(draft_law)
                
                draft_laws.append(draft_law)
                sequence += 1
                
                # é™åˆ¶å‚™ç”¨æ–¹æ³•çš„çµæœæ•¸é‡
                if len(draft_laws) >= 10:
                    break
        
        print(f"âœ… å‚™ç”¨æ–¹æ³•æ‰¾åˆ° {len(draft_laws)} ç­†è³‡æ–™")
        return draft_laws
    
    def load_history(self):
        """
        è¼‰å…¥æ­·å²è¨˜éŒ„
        
        Returns:
            list: æ­·å²è¨˜éŒ„æ¸…å–®
        """
        history_file = self.data_dir / "draft_law_history.json"
        
        if history_file.exists():
            try:
                with open(history_file, 'r', encoding='utf-8') as f:
                    history = json.load(f)
                print(f"ğŸ“š è¼‰å…¥æ­·å²è¨˜éŒ„: {len(history)} ç­†")
                return history
            except Exception as e:
                print(f"âš ï¸ è¼‰å…¥æ­·å²è¨˜éŒ„å¤±æ•—: {e}")
                return []
        else:
            print("ğŸ“ é¦–æ¬¡åŸ·è¡Œï¼Œç„¡æ­·å²è¨˜éŒ„")
            return []
    
    def compare_and_update_smart(self, current_drafts):
        """
        æ™ºèƒ½æ¯”å°å’Œæ›´æ–°é‚è¼¯
        
        Args:
            current_drafts (list): ç•¶å‰çˆ¬å–çš„è‰æ¡ˆæ¸…å–®
            
        Returns:
            tuple: (new_items, updated_items, all_history, stats)
        """
        print("\nğŸ§  åŸ·è¡Œæ™ºèƒ½æ¯”å°åˆ†æ...")
        
        history = self.load_history()
        
        # å»ºç«‹æ­·å²è³‡æ–™çš„ unique_id å°æ‡‰è¡¨
        history_dict = {item.get('unique_id'): item for item in history if item.get('unique_id')}
        current_dict = {item.get('unique_id'): item for item in current_drafts if item.get('unique_id')}
        
        print(f"ğŸ“Š æ¯”å°çµ±è¨ˆ:")
        print(f"   â€¢ ç•¶å‰ç¶²ç«™è‰æ¡ˆ: {len(current_drafts)} ç­†")
        print(f"   â€¢ æ­·å²è¨˜éŒ„è‰æ¡ˆ: {len(history)} ç­†")
        print(f"   â€¢ æ­·å²æœ‰æ•ˆID: {len(history_dict)} å€‹")
        
        # 1. æ‰¾å‡ºçœŸæ­£çš„æ–°é …ç›® (unique_id ä¸å­˜åœ¨æ–¼æ­·å²ä¸­)
        new_items = []
        for uid, draft in current_dict.items():
            if uid not in history_dict:
                new_items.append(draft)
                print(f"ğŸ†• ç™¼ç¾æ–°è‰æ¡ˆ: {draft.get('title', '')[:50]}...")
        
        # 2. æ›´æ–°ç¾æœ‰é …ç›®ç‹€æ…‹ (å¦‚æœæœ‰è®ŠåŒ–)
        updated_items = []
        for uid, current_draft in current_dict.items():
            if uid in history_dict:
                historical_draft = history_dict[uid]
                
                # æª¢æŸ¥æ˜¯å¦æœ‰é‡è¦è®ŠåŒ–
                has_changes = False
                changes = []
                
                if current_draft.get('title') != historical_draft.get('title'):
                    has_changes = True
                    changes.append("æ¨™é¡Œ")
                
                if current_draft.get('end_date') != historical_draft.get('end_date'):
                    has_changes = True
                    changes.append("çµ‚æ­¢æ—¥æœŸ")
                
                if current_draft.get('date') != historical_draft.get('date'):
                    has_changes = True
                    changes.append("å…¬å‘Šæ—¥æœŸ")
                
                if has_changes:
                    # æ›´æ–° scraped_at å’Œè®Šæ›´è¨˜éŒ„
                    current_draft['last_updated'] = datetime.now(self.tz_taipei).isoformat()
                    current_draft['changes'] = changes
                    updated_items.append(current_draft)
                    print(f"ğŸ”„ è‰æ¡ˆå·²æ›´æ–°: {current_draft.get('title', '')[:50]}... (è®Šæ›´: {', '.join(changes)})")
        
        # 3. æ¨™è¨˜ä¸å†å‡ºç¾çš„é …ç›®ç‚ºéæœŸ
        expired_count = 0
        current_uids = set(current_dict.keys())
        for uid, historical_draft in history_dict.items():
            if uid not in current_uids and historical_draft.get('status') == 'active':
                # æ¨™è¨˜ç‚ºéæœŸä½†ä¿ç•™åœ¨æ­·å²ä¸­
                historical_draft['status'] = 'expired'
                historical_draft['expired_at'] = datetime.now(self.tz_taipei).isoformat()
                expired_count += 1
                print(f"â° è‰æ¡ˆå·²éæœŸ: {historical_draft.get('title', '')[:50]}...")
        
        # 4. å»ºç«‹æ›´æ–°å¾Œçš„å®Œæ•´æ­·å²è¨˜éŒ„
        updated_history = []
        
        # åŠ å…¥æ‰€æœ‰ç¾æœ‰çš„æ´»èºé …ç›® (æ–°çš„å’Œæ›´æ–°çš„)
        for draft in current_drafts:
            # å¦‚æœæ˜¯æ›´æ–°çš„é …ç›®ï¼Œä½¿ç”¨æ›´æ–°å¾Œçš„ç‰ˆæœ¬
            uid = draft.get('unique_id')
            updated_version = next((item for item in updated_items if item.get('unique_id') == uid), draft)
            updated_history.append(updated_version)
        
        # åŠ å…¥æ­·å²ä¸­çš„éæœŸé …ç›®
        for uid, historical_draft in history_dict.items():
            if uid not in current_dict and historical_draft.get('status') == 'expired':
                updated_history.append(historical_draft)
        
        # æŒ‰æ—¥æœŸæ’åº (æœ€æ–°çš„åœ¨å‰)
        updated_history.sort(key=lambda x: x.get('date', ''), reverse=True)
        
        # å„²å­˜æ›´æ–°å¾Œçš„æ­·å²
        self.save_history(updated_history)
        
        # çµ±è¨ˆè³‡è¨Š
        stats = {
            'new_count': len(new_items),
            'updated_count': len(updated_items),
            'expired_count': expired_count,
            'active_count': len(current_drafts),
            'total_historical': len(updated_history)
        }
        
        print(f"\nğŸ“ˆ æ™ºèƒ½æ¯”å°çµæœ:")
        print(f"   ğŸ†• æ–°å¢: {stats['new_count']} ç­†")
        print(f"   ğŸ”„ æ›´æ–°: {stats['updated_count']} ç­†")
        print(f"   â° éæœŸ: {stats['expired_count']} ç­†")
        print(f"   âœ… ç•¶å‰æ´»èº: {stats['active_count']} ç­†")
        print(f"   ğŸ“š æ­·å²ç¸½è¨ˆ: {stats['total_historical']} ç­†")
        
        return new_items, updated_items, updated_history, stats
    
    def save_history(self, all_drafts):
        """
        å„²å­˜æ­·å²è¨˜éŒ„
        
        Args:
            all_drafts (list): å®Œæ•´çš„è‰æ¡ˆæ¸…å–®
        """
        history_file = self.data_dir / "draft_law_history.json"
        
        try:
            with open(history_file, 'w', encoding='utf-8') as f:
                json.dump(all_drafts, f, ensure_ascii=False, indent=2)
            print(f"âœ… æ­·å²è¨˜éŒ„å·²å„²å­˜: {history_file} ({len(all_drafts)} ç­†)")
        except Exception as e:
            print(f"âŒ å„²å­˜æ­·å²è¨˜éŒ„å¤±æ•—: {e}")
    
    def generate_comprehensive_report(self, new_items, updated_items, current_drafts, stats):
        """
        ç”Ÿæˆè©³ç´°å ±å‘Š
        
        Args:
            new_items (list): æ–°å¢é …ç›®
            updated_items (list): æ›´æ–°é …ç›®
            current_drafts (list): ç•¶å‰è‰æ¡ˆ
            stats (dict): çµ±è¨ˆè³‡æ–™
            
        Returns:
            dict: è©³ç´°å ±å‘Š
        """
        print("\nğŸ“‹ ç”Ÿæˆè©³ç´°å ±å‘Š...")
        
        report = {
            'execution_time': datetime.now(self.tz_taipei).strftime('%Y-%m-%d %H:%M:%S'),
            'source': 'MOF_DraftForum',
            'scraper_version': '2.0_enhanced',
            'target_url': self.base_url,
            'statistics': stats,
            'summary': {
                'has_new': len(new_items) > 0,
                'has_updates': len(updated_items) > 0,
                'total_changes': len(new_items) + len(updated_items),
                'execution_status': 'success'
            },
            'metadata': {
                'timezone': 'Asia/Taipei',
                'data_dir': str(self.data_dir),
                'files_generated': []
            }
        }
        
        # å„²å­˜ä¸»å ±å‘Š
        report_file = self.data_dir / "draft_report.json"
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)
        report['metadata']['files_generated'].append('draft_report.json')
        print(f"âœ… ä¸»å ±å‘Šå·²å„²å­˜: {report_file}")
        
        # å„²å­˜æ–°ç™¼ç¾çš„è‰æ¡ˆ
        if new_items:
            new_file = self.data_dir / "today_new_drafts.json"
            with open(new_file, 'w', encoding='utf-8') as f:
                json.dump(new_items, f, ensure_ascii=False, indent=2)
            report['metadata']['files_generated'].append('today_new_drafts.json')
            print(f"âœ… æ–°å¢è‰æ¡ˆæ¸…å–®å·²å„²å­˜: {new_file} ({len(new_items)} ç­†)")
        
        # å„²å­˜æ›´æ–°çš„è‰æ¡ˆ
        if updated_items:
            updated_file = self.data_dir / "today_updated_drafts.json"
            with open(updated_file, 'w', encoding='utf-8') as f:
                json.dump(updated_items, f, ensure_ascii=False, indent=2)
            report['metadata']['files_generated'].append('today_updated_drafts.json')
            print(f"âœ… æ›´æ–°è‰æ¡ˆæ¸…å–®å·²å„²å­˜: {updated_file} ({len(updated_items)} ç­†)")
        
        # å„²å­˜ç•¶å‰æ´»èºè‰æ¡ˆ
        if current_drafts:
            current_file = self.data_dir / "current_active_drafts.json"
            with open(current_file, 'w', encoding='utf-8') as f:
                json.dump(current_drafts, f, ensure_ascii=False, indent=2)
            report['metadata']['files_generated'].append('current_active_drafts.json')
            print(f"âœ… ç•¶å‰æ´»èºè‰æ¡ˆå·²å„²å­˜: {current_file} ({len(current_drafts)} ç­†)")
        
        return report
    
    def export_to_csv(self, drafts):
        """
        åŒ¯å‡ºç‚ºCSVæ ¼å¼
        
        Args:
            drafts (list): è‰æ¡ˆæ¸…å–®
            
        Returns:
            Path: CSVæª”æ¡ˆè·¯å¾‘
        """
        if not drafts:
            print("âš ï¸ ç„¡è³‡æ–™å¯åŒ¯å‡ºç‚ºCSV")
            return None
            
        print(f"ğŸ’¾ åŒ¯å‡º {len(drafts)} ç­†è³‡æ–™ç‚ºCSV...")
        
        try:
            df = pd.DataFrame(drafts)
            
            # é‡æ–°æ’åˆ—æ¬„ä½é †åºï¼Œç¢ºä¿é‡è¦è³‡è¨Šåœ¨å‰
            preferred_columns = [
                'unique_id', 'sequence', 'status', 'date', 'title', 
                'url', 'end_date', 'roc_date', 'roc_end_date',
                'scraped_at', 'source', 'scraper_version'
            ]
            
            # åªä¿ç•™å­˜åœ¨çš„æ¬„ä½
            available_columns = [col for col in preferred_columns if col in df.columns]
            
            # åŠ å…¥å…¶ä»–æ¬„ä½
            other_columns = [col for col in df.columns if col not in available_columns]
            final_columns = available_columns + other_columns
            
            df = df[final_columns]
            
            # ç”Ÿæˆæª”æ¡ˆåç¨±
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            csv_file = self.data_dir / f'draft_laws_{timestamp}.csv'
            
            # å„²å­˜CSV (ä½¿ç”¨UTF-8 BOMç¢ºä¿ä¸­æ–‡æ­£ç¢ºé¡¯ç¤º)
            df.to_csv(csv_file, index=False, encoding='utf-8-sig')
            
            print(f"âœ… CSVæª”æ¡ˆå·²å„²å­˜: {csv_file}")
            print(f"ğŸ“Š CSVåŒ…å«æ¬„ä½: {len(final_columns)} å€‹")
            print(f"ğŸ“‹ CSVåŒ…å«è¨˜éŒ„: {len(df)} ç­†")
            
            return csv_file
            
        except Exception as e:
            print(f"âŒ CSVåŒ¯å‡ºå¤±æ•—: {e}")
            return None
    
    def print_summary(self, new_items, updated_items, stats):
        """
        åˆ—å°åŸ·è¡Œæ‘˜è¦
        
        Args:
            new_items (list): æ–°å¢é …ç›®
            updated_items (list): æ›´æ–°é …ç›®
            stats (dict): çµ±è¨ˆè³‡æ–™
        """
        print("\n" + "="*60)
        print("ğŸ“Š åŸ·è¡Œæ‘˜è¦")
        print("="*60)
        
        print(f"ğŸ• åŸ·è¡Œæ™‚é–“: {datetime.now(self.tz_taipei).strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"ğŸŒ æ™‚å€: Asia/Taipei (UTC+8)")
        print(f"ğŸ“¡ ä¾†æº: {self.base_url}")
        print()
        
        print("ğŸ“ˆ çµ±è¨ˆè³‡æ–™:")
        print(f"   ğŸ†• æ–°å¢è‰æ¡ˆ: {stats['new_count']} ç­†")
        print(f"   ğŸ”„ æ›´æ–°è‰æ¡ˆ: {stats['updated_count']} ç­†")
        print(f"   â° éæœŸè‰æ¡ˆ: {stats['expired_count']} ç­†")
        print(f"   âœ… ç•¶å‰æ´»èº: {stats['active_count']} ç­†")
        print(f"   ğŸ“š æ­·å²ç¸½è¨ˆ: {stats['total_historical']} ç­†")
        print()
        
        # é¡¯ç¤ºæ–°å¢è‰æ¡ˆè©³æƒ…
        if new_items:
            print("ğŸ†• æ–°å¢è‰æ¡ˆè©³æƒ…:")
            for i, item in enumerate(new_items[:3], 1):  # åªé¡¯ç¤ºå‰3ç­†
                print(f"   {i}. {item.get('title', 'ç„¡æ¨™é¡Œ')[:60]}...")
                if item.get('end_date'):
                    print(f"      é å‘Šçµ‚æ­¢: {item.get('end_date')}")
            if len(new_items) > 3:
                print(f"   ...é‚„æœ‰ {len(new_items) - 3} ç­†æ–°å¢è‰æ¡ˆ")
            print()
        
        # é¡¯ç¤ºæ›´æ–°è‰æ¡ˆè©³æƒ…
        if updated_items:
            print("ğŸ”„ æ›´æ–°è‰æ¡ˆè©³æƒ…:")
            for i, item in enumerate(updated_items[:3], 1):  # åªé¡¯ç¤ºå‰3ç­†
                print(f"   {i}. {item.get('title', 'ç„¡æ¨™é¡Œ')[:60]}...")
                if item.get('changes'):
                    print(f"      è®Šæ›´é …ç›®: {', '.join(item.get('changes'))}")
            if len(updated_items) > 3:
                print(f"   ...é‚„æœ‰ {len(updated_items) - 3} ç­†æ›´æ–°è‰æ¡ˆ")
            print()
        
        total_changes = len(new_items) + len(updated_items)
        if total_changes > 0:
            print(f"ğŸ‰ ç™¼ç¾ {total_changes} é …é‡è¦è®ŠåŒ–!")
        else:
            print("âœ¨ æ²’æœ‰ç™¼ç¾æ–°çš„è®ŠåŒ–ï¼Œç³»çµ±é‹ä½œæ­£å¸¸")
        
        print("="*60)

def main():
    """
    ä¸»ç¨‹å¼å…¥å£
    """
    print("=" * 60)
    print("ğŸ“œ æ”¹é€²ç‰ˆè²¡æ”¿éƒ¨æ³•è¦è‰æ¡ˆçˆ¬èŸ²ç³»çµ±")
    print("ğŸ”— ç›®æ¨™ç¶²ç«™: https://law-out.mof.gov.tw/DraftForum.aspx")
    print(f"ğŸ• åŸ·è¡Œæ™‚é–“: {datetime.now()}")
    print(f"ğŸš€ ç‰ˆæœ¬: 2.0 Enhanced")
    print("=" * 60)
    
    try:
        # å»ºç«‹çˆ¬èŸ²å¯¦ä¾‹
        scraper = ImprovedDraftLawScraper()
        
        # åŸ·è¡Œçˆ¬å–
        print("\nğŸ” éšæ®µä¸€: çˆ¬å–æ³•è¦è‰æ¡ˆ")
        print("-" * 40)
        current_drafts = scraper.fetch_draft_laws()
        
        if not current_drafts:
            print("âŒ æœªèƒ½ç²å–ä»»ä½•æ³•è¦è‰æ¡ˆè³‡æ–™")
            print("\nğŸ”§ å¯èƒ½çš„è§£æ±ºæ–¹æ¡ˆ:")
            print("   1. æª¢æŸ¥ç¶²è·¯é€£ç·š")
            print("   2. ç¢ºèªç›®æ¨™ç¶²ç«™å¯æ­£å¸¸è¨ªå•")
            print("   3. ç¨å¾Œé‡è©¦")
            return False
        
        print(f"âœ… çˆ¬å–éšæ®µå®Œæˆï¼Œå…±ç²å¾— {len(current_drafts)} ç­†ç•¶å‰æ´»èºè‰æ¡ˆ")
        
        # æ™ºèƒ½æ¯”å°å’Œæ›´æ–°
        print("\nğŸ§  éšæ®µäºŒ: æ™ºèƒ½æ¯”å°åˆ†æ")
        print("-" * 40)
        new_items, updated_items, all_history, stats = scraper.compare_and_update_smart(current_drafts)
        
        # ç”Ÿæˆå ±å‘Š
        print("\nğŸ“‹ éšæ®µä¸‰: ç”Ÿæˆè©³ç´°å ±å‘Š")
        print("-" * 40)
        report = scraper.generate_comprehensive_report(new_items, updated_items, current_drafts, stats)
        
        # åŒ¯å‡ºè³‡æ–™
        print("\nğŸ’¾ éšæ®µå››: åŒ¯å‡ºè³‡æ–™æª”æ¡ˆ")
        print("-" * 40)
        csv_file = scraper.export_to_csv(all_history)
        
        # é¡¯ç¤ºæœ€çµ‚æ‘˜è¦
        scraper.print_summary(new_items, updated_items, stats)
        
        # æª”æ¡ˆæ¸…å–®
        print("\nğŸ“ ç”Ÿæˆçš„æª”æ¡ˆ:")
        for filename in report['metadata']['files_generated']:
            file_path = scraper.data_dir / filename
            if file_path.exists():
                size = file_path.stat().st_size
                print(f"   âœ… {filename} ({size} bytes)")
        
        if csv_file:
            print(f"   âœ… {csv_file.name} ({csv_file.stat().st_size} bytes)")
        
        print(f"\nğŸ¯ åŸ·è¡ŒæˆåŠŸå®Œæˆï¼")
        print(f"ğŸ“Š æ•¸æ“šç›®éŒ„: {scraper.data_dir}")
        
        return True
        
    except KeyboardInterrupt:
        print("\nâš ï¸ ç”¨æˆ¶ä¸­æ–·åŸ·è¡Œ")
        return False
    except Exception as e:
        print(f"\nâŒ åŸ·è¡Œéç¨‹ç™¼ç”Ÿæœªé æœŸéŒ¯èª¤: {e}")
        print("\nğŸ”§ å»ºè­°:")
        print("   1. æª¢æŸ¥éŒ¯èª¤è¨Šæ¯")
        print("   2. ç¢ºèªç¶²è·¯é€£ç·šæ­£å¸¸")
        print("   3. å¦‚å•é¡ŒæŒçºŒï¼Œè«‹å›å ±éŒ¯èª¤è¨Šæ¯")
        return False

if __name__ == "__main__":
    success = main()
    
    # è¨­å®šé€€å‡ºç¢¼
    exit(0 if success else 1)
