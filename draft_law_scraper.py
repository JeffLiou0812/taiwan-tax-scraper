#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
è²¡æ”¿éƒ¨æ³•è¦è‰æ¡ˆçˆ¬èŸ² - URLä¿®æ­£ç‰ˆ
ç¢ºä¿æ¯å€‹è‰æ¡ˆéƒ½æœ‰å¯ç”¨çš„é€£çµ

ç›®æ¨™ç¶²ç«™: https://law-out.mof.gov.tw/DraftForum.aspx
ç‰ˆæœ¬: 4.0 URL-Fixed
æ›´æ–°æ—¥æœŸ: 2025-08-21
"""

import requests
import json
import pandas as pd
from datetime import datetime, timezone, timedelta
import re
from pathlib import Path
import hashlib
import time
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse, quote
import logging
from typing import Dict, List, Tuple, Optional

class DraftLawScraperFixed:
    """æ³•è¦è‰æ¡ˆçˆ¬èŸ² - URLä¿®æ­£ç‰ˆ"""
    
    def __init__(self, data_dir="data", debug=True):
        """åˆå§‹åŒ–çˆ¬èŸ²"""
        self.base_url = "https://law-out.mof.gov.tw"
        self.draft_page_url = "https://law-out.mof.gov.tw/DraftForum.aspx"
        self.data_dir = Path(data_dir)
        self.data_dir.mkdir(exist_ok=True)
        
        # è¨­å®šæ—¥èªŒ
        self.setup_logging(debug)
        
        # è«‹æ±‚æ¨™é ­
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'zh-TW,zh;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1'
        }
        
        self.tz_taipei = timezone(timedelta(hours=8))
        self.session = requests.Session()
        self.session.headers.update(self.headers)
    
    def setup_logging(self, debug: bool):
        """è¨­å®šæ—¥èªŒç³»çµ±"""
        log_level = logging.DEBUG if debug else logging.INFO
        logging.basicConfig(
            level=log_level,
            format='%(asctime)s - %(levelname)s - %(message)s',
            datefmt='%Y-%m-%d %H:%M:%S'
        )
        self.logger = logging.getLogger(__name__)
    
    def fetch_draft_laws(self) -> List[Dict]:
        """çˆ¬å–æ³•è¦è‰æ¡ˆåˆ—è¡¨"""
        all_drafts = []
        
        try:
            self.logger.info("é–‹å§‹çˆ¬å–æ³•è¦è‰æ¡ˆ...")
            
            # ç™¼é€è«‹æ±‚
            response = self.session.get(self.draft_page_url, timeout=30)
            
            if response.status_code != 200:
                self.logger.error(f"HTTP {response.status_code}")
                return []
            
            # è§£æé é¢
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # å°‹æ‰¾è¡¨æ ¼
            tables = soup.find_all('table')
            
            for table in tables:
                rows = table.find_all('tr')
                
                for row in rows[1:]:  # è·³éæ¨™é¡Œåˆ—
                    cells = row.find_all('td')
                    
                    if len(cells) >= 2:
                        draft = self.extract_draft_from_cells(cells, soup)
                        if draft:
                            all_drafts.append(draft)
            
            # å¦‚æœæ²’æœ‰æ‰¾åˆ°è¡¨æ ¼è³‡æ–™ï¼Œå˜—è©¦å…¶ä»–æ–¹å¼
            if not all_drafts:
                self.logger.warning("æœªå¾è¡¨æ ¼æ‰¾åˆ°è³‡æ–™ï¼Œå˜—è©¦å…¶ä»–è§£ææ–¹å¼...")
                # å°‹æ‰¾å¯èƒ½çš„è‰æ¡ˆé …ç›®
                items = soup.find_all(['div', 'li'], class_=re.compile(r'item|draft|law'))
                for item in items:
                    draft = self.extract_draft_from_element(item)
                    if draft:
                        all_drafts.append(draft)
            
            self.logger.info(f"æˆåŠŸçˆ¬å– {len(all_drafts)} ç­†æ³•è¦è‰æ¡ˆ")
            
        except Exception as e:
            self.logger.error(f"çˆ¬å–å¤±æ•—: {e}")
            import traceback
            traceback.print_exc()
        
        return all_drafts
    
    def extract_draft_from_cells(self, cells, full_soup) -> Optional[Dict]:
        """å¾è¡¨æ ¼å„²å­˜æ ¼æå–è‰æ¡ˆè³‡è¨Š"""
        try:
            draft = {
                'source': 'MOF_Taiwan_Draft',
                'scrape_time': datetime.now(self.tz_taipei).isoformat()
            }
            
            # æå–æ—¥æœŸ
            if len(cells) > 0:
                date_text = cells[0].get_text(strip=True)
                draft['announcement_date_roc'] = date_text
                draft['announcement_date'] = self.convert_roc_date(date_text)
            
            # æå–æ¨™é¡Œå’Œé€£çµ
            if len(cells) > 1:
                title_cell = cells[1]
                draft['title'] = title_cell.get_text(strip=True)
                
                # å°‹æ‰¾é€£çµ
                link = title_cell.find('a')
                if link:
                    href = link.get('href', '')
                    onclick = link.get('onclick', '')
                    
                    # å˜—è©¦å„ç¨®æ–¹å¼æå–URL
                    extracted_url = self.extract_url_from_link(href, onclick, full_soup)
                    draft['url'] = extracted_url
                else:
                    # æ²’æœ‰ç›´æ¥é€£çµï¼Œç¨å¾Œæœƒç”Ÿæˆæœå°‹é€£çµ
                    draft['url'] = None
            
            # æå–æˆªæ­¢æ—¥æœŸ
            if len(cells) > 2:
                end_date_text = cells[2].get_text(strip=True)
                draft['end_date_roc'] = end_date_text
                draft['end_date'] = self.convert_roc_date(end_date_text)
                draft['status'] = self.check_status(draft['end_date'])
            else:
                draft['status'] = 'é€²è¡Œä¸­'
            
            # å¦‚æœé‚„æ˜¯æ²’æœ‰URLï¼Œç”Ÿæˆæ™ºèƒ½é€£çµ
            if not draft.get('url'):
                draft['url'] = self.generate_smart_url(draft)
                draft['url_type'] = 'generated'
            else:
                draft['url_type'] = 'original'
            
            # ç”Ÿæˆå”¯ä¸€ID
            if draft.get('title'):
                draft['id'] = self.generate_unique_id(draft)
                return draft
                
        except Exception as e:
            self.logger.debug(f"æå–éŒ¯èª¤: {e}")
        
        return None
    
    def extract_draft_from_element(self, element) -> Optional[Dict]:
        """å¾HTMLå…ƒç´ æå–è‰æ¡ˆè³‡è¨Š"""
        try:
            draft = {
                'source': 'MOF_Taiwan_Draft',
                'scrape_time': datetime.now(self.tz_taipei).isoformat()
            }
            
            text = element.get_text(strip=True)
            
            # æå–æ¨™é¡Œ
            draft['title'] = text[:200] if len(text) > 200 else text
            
            # æå–æ—¥æœŸ
            date_match = re.search(r'\d{3}\.\d{1,2}\.\d{1,2}', text)
            if date_match:
                draft['announcement_date_roc'] = date_match.group()
                draft['announcement_date'] = self.convert_roc_date(date_match.group())
            
            # å°‹æ‰¾é€£çµ
            link = element.find('a')
            if link:
                href = link.get('href', '')
                draft['url'] = self.process_url(href)
            else:
                draft['url'] = self.generate_smart_url(draft)
            
            draft['status'] = 'é€²è¡Œä¸­'
            
            if draft.get('title'):
                draft['id'] = self.generate_unique_id(draft)
                return draft
                
        except Exception as e:
            self.logger.debug(f"å…ƒç´ æå–éŒ¯èª¤: {e}")
        
        return None
    
    def extract_url_from_link(self, href, onclick, soup) -> Optional[str]:
        """å¾å„ç¨®å±¬æ€§ä¸­æå–URL"""
        # æ–¹æ³•1: ç›´æ¥ä½¿ç”¨href
        if href and href != '#' and not href.startswith('javascript:'):
            return self.process_url(href)
        
        # æ–¹æ³•2: å¾onclickä¸­æå–
        if onclick:
            # å°‹æ‰¾window.openæˆ–é¡ä¼¼çš„URL
            url_patterns = [
                r"window\.open\(['\"]([^'\"]+)['\"]",
                r"location\.href=['\"]([^'\"]+)['\"]",
                r"['\"]([^'\"]*join\.gov\.tw[^'\"]+)['\"]",
                r"['\"]([^'\"]*https?://[^'\"]+)['\"]"
            ]
            
            for pattern in url_patterns:
                match = re.search(pattern, onclick)
                if match:
                    return self.process_url(match.group(1))
        
        # æ–¹æ³•3: æœå°‹é é¢ä¸­çš„join.gov.twé€£çµ
        join_links = soup.find_all('a', href=re.compile(r'join\.gov\.tw'))
        if join_links:
            # è¿”å›ç¬¬ä¸€å€‹æ‰¾åˆ°çš„join.gov.twé€£çµ
            for link in join_links:
                if link.get('href'):
                    return link.get('href')
        
        return None
    
    def process_url(self, url: str) -> str:
        """è™•ç†å’Œæ¨™æº–åŒ–URL"""
        if not url:
            return ""
        
        url = url.strip()
        
        # è™•ç†ç›¸å°è·¯å¾‘
        if not url.startswith(('http://', 'https://')):
            if url.startswith('/'):
                return f"{self.base_url}{url}"
            else:
                return f"{self.base_url}/{url}"
        
        return url
    
    def generate_smart_url(self, draft: Dict) -> str:
        """
        ç”Ÿæˆæ™ºèƒ½URL
        å¦‚æœæ²’æœ‰æ‰¾åˆ°ç›´æ¥é€£çµï¼Œç”Ÿæˆä¸€å€‹æœ‰ç”¨çš„æ›¿ä»£é€£çµ
        """
        title = draft.get('title', '')
        
        # å„ªå…ˆé †åºï¼š
        # 1. å¦‚æœæ¨™é¡ŒåŒ…å«ç‰¹å®šé—œéµå­—ï¼Œå¯èƒ½åœ¨join.gov.tw
        if 'æ„è¦‹' in title or 'å…¬å‘Š' in title or 'é å‘Š' in title:
            # ç”Ÿæˆjoin.gov.twæœå°‹é€£çµ
            search_query = quote(title[:50])  # é™åˆ¶é•·åº¦
            return f"https://join.gov.tw/policies/search?q={search_query}"
        
        # 2. ç”ŸæˆGoogleæœå°‹é€£çµï¼ˆæœå°‹æ¨™é¡Œ+ç¶²ç«™ï¼‰
        search_terms = f"{title} site:law-out.mof.gov.tw OR site:join.gov.tw OR site:mof.gov.tw"
        google_search = f"https://www.google.com/search?q={quote(search_terms)}"
        
        self.logger.info(f"ç‚ºã€Œ{title[:30]}...ã€ç”Ÿæˆæœå°‹é€£çµ")
        
        return google_search
    
    def convert_roc_date(self, roc_date_str: str) -> Optional[str]:
        """è½‰æ›æ°‘åœ‹å¹´ç‚ºè¥¿å…ƒå¹´"""
        if not roc_date_str:
            return None
        
        try:
            # ç§»é™¤å¤šé¤˜å­—ç¬¦
            date_str = roc_date_str.strip()
            
            # å˜—è©¦ä¸åŒçš„æ ¼å¼
            patterns = [
                r'(\d{2,3})\.(\d{1,2})\.(\d{1,2})',
                r'(\d{2,3})/(\d{1,2})/(\d{1,2})',
                r'(\d{2,3})å¹´(\d{1,2})æœˆ(\d{1,2})æ—¥'
            ]
            
            for pattern in patterns:
                match = re.search(pattern, date_str)
                if match:
                    year = int(match.group(1)) + 1911
                    month = int(match.group(2))
                    day = int(match.group(3))
                    return f"{year}-{month:02d}-{day:02d}"
        except:
            pass
        
        return None
    
    def check_status(self, end_date_str: str) -> str:
        """æª¢æŸ¥è‰æ¡ˆç‹€æ…‹"""
        if not end_date_str:
            return "é€²è¡Œä¸­"
        
        try:
            end_date = datetime.strptime(end_date_str, '%Y-%m-%d')
            if end_date < datetime.now():
                return "å·²çµæŸ"
            else:
                return "é€²è¡Œä¸­"
        except:
            return "æœªçŸ¥"
    
    def generate_unique_id(self, draft: Dict) -> str:
        """ç”Ÿæˆå”¯ä¸€è­˜åˆ¥ç¢¼"""
        content = f"{draft.get('title', '')}{draft.get('announcement_date', '')}"
        return hashlib.md5(content.encode('utf-8')).hexdigest()[:12]
    
    def compare_and_update(self, new_drafts: List[Dict]) -> Tuple[List[Dict], List[Dict]]:
        """æ¯”å°æ­·å²è¨˜éŒ„"""
        history_file = self.data_dir / "draft_history.json"
        
        # è®€å–æ­·å²
        if history_file.exists():
            try:
                with open(history_file, 'r', encoding='utf-8') as f:
                    history = json.load(f)
            except:
                history = []
        else:
            history = []
        
        # æ¯”å°
        history_ids = {item['id'] for item in history if 'id' in item}
        new_items = []
        
        for draft in new_drafts:
            if draft.get('id') and draft['id'] not in history_ids:
                new_items.append(draft)
        
        # æ›´æ–°æ­·å²
        if new_items:
            history.extend(new_items)
            if len(history) > 500:  # é™åˆ¶å¤§å°
                history = history[-500:]
            
            with open(history_file, 'w', encoding='utf-8') as f:
                json.dump(history, f, ensure_ascii=False, indent=2)
        
        return new_items, history
    
    def save_results(self, drafts: List[Dict]) -> None:
        """å„²å­˜çµæœ"""
        if not drafts:
            return
        
        timestamp = datetime.now(self.tz_taipei).strftime('%Y%m%d_%H%M%S')
        
        # JSON
        json_file = self.data_dir / f'drafts_{timestamp}.json'
        with open(json_file, 'w', encoding='utf-8') as f:
            json.dump(drafts, f, ensure_ascii=False, indent=2)
        
        # CSV
        csv_file = self.data_dir / f'drafts_{timestamp}.csv'
        df = pd.DataFrame(drafts)
        df.to_csv(csv_file, index=False, encoding='utf-8-sig')
        
        self.logger.info(f"è³‡æ–™å·²å„²å­˜: {json_file.name} å’Œ {csv_file.name}")
    
    def generate_report(self, new_drafts: List[Dict], total_drafts: List[Dict]) -> Dict:
        """ç”Ÿæˆå ±å‘Š"""
        report = {
            'execution_time': datetime.now(self.tz_taipei).isoformat(),
            'total_drafts': len(total_drafts),
            'new_drafts': len(new_drafts),
            'has_new': len(new_drafts) > 0,
            'status_summary': {},
            'url_statistics': {
                'with_original_url': sum(1 for d in total_drafts if d.get('url_type') == 'original'),
                'with_generated_url': sum(1 for d in total_drafts if d.get('url_type') == 'generated'),
                'total': len(total_drafts)
            }
        }
        
        # çµ±è¨ˆç‹€æ…‹
        if total_drafts:
            df = pd.DataFrame(total_drafts)
            if 'status' in df.columns:
                report['status_summary'] = df['status'].value_counts().to_dict()
        
        # å„²å­˜å ±å‘Š
        report_file = self.data_dir / "draft_report.json"
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)
        
        return report

def main():
    """ä¸»ç¨‹å¼"""
    print("="*60)
    print("ğŸ›ï¸ è²¡æ”¿éƒ¨æ³•è¦è‰æ¡ˆçˆ¬èŸ² - URLä¿®æ­£ç‰ˆ")
    print(f"ğŸ• åŸ·è¡Œæ™‚é–“: {datetime.now()}")
    print("ğŸ”— ç¢ºä¿æ¯å€‹è‰æ¡ˆéƒ½æœ‰å¯ç”¨é€£çµ")
    print("="*60)
    
    try:
        scraper = DraftLawScraperFixed()
        
        # çˆ¬å–
        print("\nğŸ“¡ é–‹å§‹çˆ¬å–æ³•è¦è‰æ¡ˆ...")
        drafts = scraper.fetch_draft_laws()
        
        if not drafts:
            print("âš ï¸ æœªç²å–ä»»ä½•è‰æ¡ˆè³‡æ–™")
            scraper.generate_report([], [])
            return
        
        print(f"\nâœ… æˆåŠŸçˆ¬å– {len(drafts)} ç­†è‰æ¡ˆ")
        
        # é¡¯ç¤ºURLçµ±è¨ˆ
        original_urls = sum(1 for d in drafts if d.get('url_type') == 'original')
        generated_urls = sum(1 for d in drafts if d.get('url_type') == 'generated')
        
        print(f"\nğŸ”— URL çµ±è¨ˆ:")
        print(f"   â€¢ åŸå§‹é€£çµ: {original_urls} ç­†")
        print(f"   â€¢ ç”Ÿæˆé€£çµ: {generated_urls} ç­†")
        
        # é è¦½
        print("\nğŸ“‹ è³‡æ–™é è¦½:")
        for i, draft in enumerate(drafts[:3], 1):
            print(f"\n  {i}. {draft.get('title', 'N/A')[:50]}...")
            print(f"     URLé¡å‹: {draft.get('url_type', 'N/A')}")
            if draft.get('url'):
                print(f"     é€£çµ: {draft['url'][:60]}...")
        
        # æ¯”å°æ­·å²
        print("\nğŸ“Š æ¯”å°æ­·å²è¨˜éŒ„...")
        new_items, history = scraper.compare_and_update(drafts)
        
        if new_items:
            print(f"\nğŸ†• ç™¼ç¾ {len(new_items)} ç­†æ–°è‰æ¡ˆ!")
        else:
            print("\nâœ¨ æ²’æœ‰æ–°è‰æ¡ˆ")
        
        # å„²å­˜
        print("\nğŸ’¾ å„²å­˜è³‡æ–™...")
        scraper.save_results(drafts)
        
        # å ±å‘Š
        print("\nğŸ“‹ ç”Ÿæˆå ±å‘Š...")
        report = scraper.generate_report(new_items, drafts)
        
        print("\nâœ… åŸ·è¡Œå®Œæˆï¼")
        print("ğŸ”— æ‰€æœ‰è‰æ¡ˆéƒ½å·²ç¢ºä¿æœ‰å¯ç”¨é€£çµ")
        
    except Exception as e:
        print(f"\nâŒ åŸ·è¡Œå¤±æ•—: {e}")
        import traceback
        traceback.print_exc()
        exit(1)

if __name__ == "__main__":
    main()
