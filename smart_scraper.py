import requests
from bs4 import BeautifulSoup
import pandas as pd
import json
from datetime import datetime
import os
import time
import random

class SmartTaxScraper:
    def __init__(self):
        print("ğŸ¤– æ™ºæ…§ç¨…å‹™çˆ¬èŸ² v1.0 å•Ÿå‹•")
        print("="*60)
        
        self.data_dir = "data"
        if not os.path.exists(self.data_dir):
            os.makedirs(self.data_dir)
        
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'zh-TW,zh;q=0.9,en;q=0.8',
            'Referer': 'https://law.dot.gov.tw/'
        })
        
        self.base_url = "https://law.dot.gov.tw"
        self.history_file = os.path.join(self.data_dir, "smart_history.json")
        self.new_file = os.path.join(self.data_dir, "today_new.json")
        self.report_file = os.path.join(self.data_dir, "daily_report.json")
    
    def fetch_rulings(self):
        """æ“·å–æœ€æ–°å‡½é‡‹"""
        print("ğŸ“¡ é€£ç·šåˆ°è²¡æ”¿éƒ¨ç¶²ç«™...")
        
        try:
            # å…ˆè¨ªå•ä¸»é å»ºç«‹session
            self.session.get(f"{self.base_url}/", timeout=10)
            time.sleep(random.uniform(1, 2))
            
            # æŸ¥è©¢å‡½é‡‹é é¢
            url = f"{self.base_url}/law-ch/home.jsp"
            params = {
                'id': '18',
                'mcustomize': 'newlaw_list.jsp',
                'sort': '1',
                'up_down': 'D'
            }
            
            response = self.session.get(url, params=params, timeout=30)
            
            if response.status_code == 200:
                response.encoding = 'utf-8'
                soup = BeautifulSoup(response.text, 'html.parser')
                print("âœ… æˆåŠŸé€£ç·š")
                return self.parse_content(soup)
            else:
                print(f"âŒ é€£ç·šå¤±æ•—: {response.status_code}")
                return []
                
        except Exception as e:
            print(f"âŒ éŒ¯èª¤: {str(e)}")
            return []
    
    def parse_content(self, soup):
        """è§£æç¶²é å…§å®¹"""
        rulings = []
        
        # æ–¹æ³•1: å¾è¡¨æ ¼æ“·å–
        tables = soup.find_all('table')
        for table in tables:
            rows = table.find_all('tr')[1:]  # è·³éæ¨™é¡Œåˆ—
            for row in rows:
                cells = row.find_all('td')
                if len(cells) >= 2:
                    ruling = self.extract_ruling_from_row(row, cells)
                    if ruling:
                        rulings.append(ruling)
        
        # æ–¹æ³•2: å¾é€£çµæ“·å–ï¼ˆå¦‚æœè¡¨æ ¼æ²’è³‡æ–™ï¼‰
        if not rulings:
            links = soup.find_all('a', href=True)
            for link in links:
                if self.is_ruling_link(link):
                    ruling = self.extract_ruling_from_link(link)
                    if ruling:
                        rulings.append(ruling)
        
        print(f"ğŸ“Š æ‰¾åˆ° {len(rulings)} ç­†å‡½é‡‹")
        return rulings
    
    def extract_ruling_from_row(self, row, cells):
        """å¾è¡¨æ ¼åˆ—æå–å‡½é‡‹è³‡è¨Š"""
        ruling = {}
        
        for i, cell in enumerate(cells):
            text = cell.get_text(strip=True)
            if text:
                if i == 0:
                    ruling['number'] = text
                elif i == 1:
                    ruling['date'] = text
                elif i == 2:
                    ruling['title'] = text
        
        # å°‹æ‰¾é€£çµ
        link = row.find('a', href=True)
        if link:
            ruling['url'] = self.make_full_url(link['href'])
            if not ruling.get('title'):
                ruling['title'] = link.get_text(strip=True)
        
        if ruling.get('title') or ruling.get('number'):
            ruling['found_at'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
            ruling['id'] = self.generate_id(ruling)
            return ruling
        
        return None
    
    def extract_ruling_from_link(self, link):
        """å¾é€£çµæå–å‡½é‡‹è³‡è¨Š"""
        text = link.get_text(strip=True)
        href = link.get('href', '')
        
        ruling = {
            'title': text,
            'url': self.make_full_url(href),
            'found_at': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        }
        ruling['id'] = self.generate_id(ruling)
        return ruling
    
    def is_ruling_link(self, link):
        """åˆ¤æ–·æ˜¯å¦ç‚ºå‡½é‡‹é€£çµ"""
        text = link.get_text(strip=True)
        href = link.get('href', '')
        
        keywords = ['å‡½', 'ä»¤', 'é‡‹', 'ç¨…', 'law']
        return any(kw in text or kw in href.lower() for kw in keywords) and len(text) > 5
    
    def make_full_url(self, url):
        """å»ºç«‹å®Œæ•´URL"""
        if url and not url.startswith('http'):
            return self.base_url + url
        return url
    
    def generate_id(self, ruling):
        """ç”¢ç”Ÿå”¯ä¸€ID"""
        # ä½¿ç”¨æ¨™é¡Œæˆ–å­—è™Ÿä½œç‚ºID
        text = ruling.get('title', '') or ruling.get('number', '')
        return str(hash(text))[-8:]
    
    def check_new_items(self, current_rulings):
        """æª¢æŸ¥æ–°å‡½é‡‹"""
        print("\nğŸ” æ¯”å°æ­·å²è¨˜éŒ„...")
        
        # è¼‰å…¥æ­·å²è¨˜éŒ„
        history = {}
        if os.path.exists(self.history_file):
            with open(self.history_file, 'r', encoding='utf-8') as f:
                history_list = json.load(f)
                history = {item['id']: item for item in history_list if 'id' in item}
                print(f"ğŸ“š æ­·å²è¨˜éŒ„: {len(history)} ç­†")
        else:
            print("ğŸ“š é¦–æ¬¡åŸ·è¡Œï¼Œå»ºç«‹æ­·å²è¨˜éŒ„")
        
        # æ‰¾å‡ºæ–°å‡½é‡‹
        new_rulings = []
        for ruling in current_rulings:
            if ruling['id'] not in history:
                new_rulings.append(ruling)
                print(f"  ğŸ†• æ–°ç™¼ç¾: {ruling.get('title', 'N/A')[:40]}...")
        
        # æ›´æ–°æ­·å²è¨˜éŒ„
        with open(self.history_file, 'w', encoding='utf-8') as f:
            json.dump(current_rulings, f, ensure_ascii=False, indent=2)
        
        return new_rulings
    
    def save_new_rulings(self, new_rulings):
        """å„²å­˜æ–°å‡½é‡‹"""
        # å„²å­˜ä»Šæ—¥æ–°å‡½é‡‹
        with open(self.new_file, 'w', encoding='utf-8') as f:
            json.dump(new_rulings, f, ensure_ascii=False, indent=2)
        
        if new_rulings:
            # å„²å­˜å¸¶æ™‚é–“æˆ³çš„æª”æ¡ˆ
            timestamp = datetime.now().strftime('%Y%m%d')
            filename = os.path.join(self.data_dir, f'new_{timestamp}.json')
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(new_rulings, f, ensure_ascii=False, indent=2)
            
            # å„²å­˜CSVç‰ˆæœ¬
            csv_file = os.path.join(self.data_dir, f'new_{timestamp}.csv')
            df = pd.DataFrame(new_rulings)
            df.to_csv(csv_file, index=False, encoding='utf-8-sig')
            
            print(f"ğŸ’¾ æ–°å‡½é‡‹å·²å„²å­˜: {filename}")
    
    def generate_report(self, all_rulings, new_rulings):
        """ç”¢ç”ŸåŸ·è¡Œå ±å‘Š"""
        report = {
            'execution_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
            'total_checked': len(all_rulings),
            'new_count': len(new_rulings),
            'status': 'success',
            'has_new': len(new_rulings) > 0,
            'new_titles': [r.get('title', 'N/A')[:50] for r in new_rulings[:5]]
        }
        
        with open(self.report_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)
        
        print("\n" + "="*60)
        print("ğŸ“Š åŸ·è¡Œå ±å‘Š")
        print("="*60)
        print(f"åŸ·è¡Œæ™‚é–“: {report['execution_time']}")
        print(f"æª¢æŸ¥æ•¸é‡: {report['total_checked']} ç­†")
        print(f"æ–°å¢å‡½é‡‹: {report['new_count']} ç­†")
        
        if new_rulings:
            print("\nğŸ“¢ æ–°å‡½é‡‹æ‘˜è¦:")
            for i, ruling in enumerate(new_rulings[:5], 1):
                print(f"{i}. {ruling.get('title', 'N/A')[:50]}")
                if ruling.get('date'):
                    print(f"   æ—¥æœŸ: {ruling['date']}")
        else:
            print("\nâœ¨ ä»Šå¤©æ²’æœ‰æ–°å‡½é‡‹")
        
        return report

def main():
    scraper = SmartTaxScraper()
    
    # æ“·å–å‡½é‡‹
    all_rulings = scraper.fetch_rulings()
    
    if all_rulings:
        # æª¢æŸ¥æ–°å‡½é‡‹
        new_rulings = scraper.check_new_items(all_rulings)
        
        # å„²å­˜æ–°å‡½é‡‹
        scraper.save_new_rulings(new_rulings)
        
        # ç”¢ç”Ÿå ±å‘Š
        report = scraper.generate_report(all_rulings, new_rulings)
        
        # è¨­å®šé€€å‡ºç¢¼ï¼ˆä¾›GitHub Actionsä½¿ç”¨ï¼‰
        if report['has_new']:
            print("\nğŸ¯ ä»»å‹™å®Œæˆ - æœ‰æ–°å‡½é‡‹")
        else:
            print("\nâœ… ä»»å‹™å®Œæˆ - ç„¡æ–°å‡½é‡‹")
    else:
        print("\nâš ï¸ æœªèƒ½æ“·å–è³‡æ–™ï¼Œè«‹æª¢æŸ¥ç¶²è·¯æˆ–ç¶²ç«™ç‹€æ…‹")
        # å»ºç«‹éŒ¯èª¤å ±å‘Š
        report = {
            'execution_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
            'status': 'error',
            'error': 'Failed to fetch data'
        }
        with open(scraper.report_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)
    
    print("="*60)
    print("ğŸ ç¨‹å¼çµæŸ")

if __name__ == "__main__":
    main()
