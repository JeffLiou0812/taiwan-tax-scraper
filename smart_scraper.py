#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ™ºèƒ½ç¨…å‹™å‡½é‡‹çˆ¬èŸ² - ä¿®å¾©ç‰ˆ
ä¿®å¾© URL è§£æå•é¡Œï¼Œç¢ºä¿é€£çµæ­£ç¢ºæ€§

ç›®æ¨™ç¶²ç«™: https://www.mof.gov.tw
ç‰ˆæœ¬: 1.1 (URLä¿®å¾©ç‰ˆ)
"""

import requests
import json
import pandas as pd
from datetime import datetime, timezone, timedelta
import re
from pathlib import Path
import time
from urllib.parse import urljoin, urlparse

class FixedTaxScraper:
    """ä¿®å¾©ç‰ˆç¨…å‹™å‡½é‡‹çˆ¬èŸ²"""
    
    def __init__(self, data_dir="data"):
        self.base_url = "https://www.mof.gov.tw"
        self.search_url = "https://www.mof.gov.tw/singlehtml/7e8e67631e154c389e29c336ef1ed38e?cntId=c757f46b20ed47b4aff71ddf654c55f8"
        self.data_dir = Path(data_dir)
        self.data_dir.mkdir(exist_ok=True)
        
        # è¨­å®šè«‹æ±‚æ¨™é ­
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'zh-TW,zh;q=0.8,en-US;q=0.5,en;q=0.3',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
        }
        
        # å°ç£æ™‚å€
        self.tz_taipei = timezone(timedelta(hours=8))
        
    def normalize_url(self, url):
        """
        æ­£è¦åŒ– URLï¼Œç¢ºä¿æ ¼å¼æ­£ç¢º
        
        Args:
            url (str): åŸå§‹ URL
            
        Returns:
            str: æ­£è¦åŒ–å¾Œçš„ URL
        """
        if not url:
            return ""
            
        # ç§»é™¤å¤šé¤˜çš„ç©ºç™½
        url = url.strip()
        
        # å¦‚æœæ˜¯ç›¸å°è·¯å¾‘ï¼Œè½‰æ›ç‚ºçµ•å°è·¯å¾‘
        if url.startswith('/'):
            return urljoin(self.base_url, url)
        
        # å¦‚æœä¸æ˜¯ä»¥ http é–‹é ­ï¼Œæ·»åŠ  https
        if not url.startswith(('http://', 'https://')):
            # æª¢æŸ¥æ˜¯å¦æ˜¯è²¡æ”¿éƒ¨ç›¸é—œç¶²å€
            if 'mof.gov.tw' in url or 'dot.gov.tw' in url:
                return f"https://{url}"
            else:
                return urljoin(self.base_url, url)
        
        # ä¿®å¾©å¸¸è¦‹çš„ URL å•é¡Œ
        url = url.replace('law.dot.gov.twhome.jsp', 'www.mof.gov.tw')
        
        return url
    
    def validate_url(self, url):
        """
        é©—è­‰ URL æ˜¯å¦æœ‰æ•ˆ
        
        Args:
            url (str): è¦é©—è­‰çš„ URL
            
        Returns:
            bool: URL æ˜¯å¦æœ‰æ•ˆ
        """
        try:
            result = urlparse(url)
            return all([result.scheme, result.netloc])
        except:
            return False
    
    def fetch_latest_rulings(self):
        """
        çˆ¬å–æœ€æ–°å‡½é‡‹
        
        Returns:
            list: å‡½é‡‹æ¸…å–®
        """
        print(f"ğŸ” é–‹å§‹çˆ¬å–è²¡æ”¿éƒ¨ç¨…å‹™å‡½é‡‹...")
        print(f"ğŸ“¡ ç›®æ¨™ç¶²ç«™: {self.search_url}")
        
        try:
            # ç™¼é€è«‹æ±‚
            response = requests.get(self.search_url, headers=self.headers, timeout=30)
            response.raise_for_status()
            
            print(f"âœ… ç¶²ç«™å›æ‡‰æˆåŠŸ (ç‹€æ…‹ç¢¼: {response.status_code})")
            
            # è§£æ HTML å…§å®¹
            content = response.text
            
            # ä½¿ç”¨å¤šç¨®æ–¹æ³•è§£æå‡½é‡‹è³‡è¨Š
            rulings = self._parse_rulings_comprehensive(content)
            
            print(f"âœ… æˆåŠŸè§£æ {len(rulings)} ç­†å‡½é‡‹")
            
            return rulings
            
        except requests.exceptions.RequestException as e:
            print(f"âŒ ç¶²è·¯è«‹æ±‚å¤±æ•—: {e}")
            return []
        except Exception as e:
            print(f"âŒ è§£æéç¨‹ç™¼ç”ŸéŒ¯èª¤: {e}")
            return []
    
    def _parse_rulings_comprehensive(self, content):
        """
        ç¶œåˆè§£æå‡½é‡‹è³‡è¨Š
        
        Args:
            content (str): ç¶²é å…§å®¹
            
        Returns:
            list: å‡½é‡‹æ¸…å–®
        """
        rulings = []
        current_time = datetime.now(self.tz_taipei).isoformat()
        
        # æ–¹æ³•1: è§£æè¡¨æ ¼çµæ§‹
        table_rulings = self._parse_table_format(content)
        rulings.extend(table_rulings)
        
        # æ–¹æ³•2: è§£æåˆ—è¡¨çµæ§‹
        if not rulings:
            list_rulings = self._parse_list_format(content)
            rulings.extend(list_rulings)
        
        # æ–¹æ³•3: é€šç”¨æ¨¡å¼åŒ¹é…
        if not rulings:
            pattern_rulings = self._parse_pattern_matching(content)
            rulings.extend(pattern_rulings)
        
        # ç‚ºæ¯ç­†å‡½é‡‹æ·»åŠ åŸºæœ¬è³‡è¨Šå’Œä¿®å¾© URL
        for ruling in rulings:
            ruling['scraped_at'] = current_time
            ruling['source'] = 'MOF_Taiwan'
            
            # ä¿®å¾©å’Œé©—è­‰ URL
            if 'url' in ruling and ruling['url']:
                original_url = ruling['url']
                fixed_url = self.normalize_url(original_url)
                
                if self.validate_url(fixed_url):
                    ruling['url'] = fixed_url
                    ruling['url_status'] = 'valid'
                else:
                    ruling['url_status'] = 'invalid'
                    ruling['original_url'] = original_url
                    ruling['url'] = ""  # æ¸…ç©ºç„¡æ•ˆé€£çµ
                    print(f"âš ï¸ ç„¡æ•ˆ URL å·²ä¿®å¾©: {original_url} -> å·²æ¸…ç©º")
            else:
                ruling['url_status'] = 'missing'
        
        return rulings
    
    def _parse_table_format(self, content):
        """è§£æè¡¨æ ¼æ ¼å¼çš„å‡½é‡‹"""
        rulings = []
        
        # å°‹æ‰¾è¡¨æ ¼è¡Œçš„æ¨¡å¼
        # åŒ¹é…é¡ä¼¼ï¼šæ—¥æœŸ | æ¨™é¡Œ | å­—è™Ÿ ç­‰æ ¼å¼
        table_pattern = r'(\d{2,3}å¹´\d{1,2}æœˆ\d{1,2}æ—¥).*?([^<>\n]{10,200}).*?((?:å°è²¡|è²¡ç¨…|å°è²¡ç¨…)[^<>\s]{5,30})'
        
        matches = re.findall(table_pattern, content)
        
        for match in matches:
            date_str, title, number = match
            
            ruling = {
                'date': self._convert_date(date_str),
                'title': title.strip(),
                'number': number.strip(),
                'url': "",  # ç¨å¾Œå˜—è©¦æ‰¾åˆ°å°æ‡‰é€£çµ
                'type': 'table_parsed'
            }
            
            rulings.append(ruling)
        
        return rulings
    
    def _parse_list_format(self, content):
        """è§£æåˆ—è¡¨æ ¼å¼çš„å‡½é‡‹"""
        rulings = []
        
        # å°‹æ‰¾åˆ—è¡¨é …ç›®çš„æ¨¡å¼
        list_pattern = r'<li[^>]*>.*?(\d{2,3}å¹´\d{1,2}æœˆ\d{1,2}æ—¥).*?([^<>{10,200}}).*?</li>'
        
        matches = re.findall(list_pattern, content, re.DOTALL)
        
        for match in matches:
            date_str, content_text = match
            
            # å¾å…§å®¹ä¸­æå–æ¨™é¡Œå’Œå­—è™Ÿ
            title_match = re.search(r'[^<>]{10,100}', content_text)
            number_match = re.search(r'(å°è²¡|è²¡ç¨…|å°è²¡ç¨…)[^<>\s]{5,30}', content_text)
            
            ruling = {
                'date': self._convert_date(date_str),
                'title': title_match.group().strip() if title_match else content_text[:50],
                'number': number_match.group().strip() if number_match else "",
                'url': "",
                'type': 'list_parsed'
            }
            
            rulings.append(ruling)
        
        return rulings
    
    def _parse_pattern_matching(self, content):
        """é€šç”¨æ¨¡å¼åŒ¹é…è§£æ"""
        rulings = []
        
        # æ›´å¯¬é¬†çš„åŒ¹é…æ¨¡å¼
        general_pattern = r'(\d{2,3}\.\d{1,2}\.\d{1,2}|å¹´\d{1,2}æœˆ\d{1,2}æ—¥)'
        
        date_matches = re.finditer(general_pattern, content)
        
        for date_match in list(date_matches)[:15]:  # é™åˆ¶æ•¸é‡é¿å…éå¤š
            start_pos = date_match.start()
            end_pos = min(start_pos + 300, len(content))
            context = content[start_pos:end_pos]
            
            # å˜—è©¦å¾ä¸Šä¸‹æ–‡æå–è³‡è¨Š
            title_pattern = r'[^<>{]{15,150}[ã€‚ï¼›]'
            title_match = re.search(title_pattern, context)
            
            number_pattern = r'(å°è²¡|è²¡ç¨…|å°è²¡ç¨…)[^<>\s]{5,30}'
            number_match = re.search(number_pattern, context)
            
            if title_match:  # è‡³å°‘è¦æœ‰æ¨™é¡Œæ‰ç®—æœ‰æ•ˆ
                ruling = {
                    'date': self._convert_date(date_match.group()),
                    'title': title_match.group().strip(),
                    'number': number_match.group().strip() if number_match else "",
                    'url': "",
                    'type': 'pattern_matched'
                }
                
                rulings.append(ruling)
        
        return rulings
    
    def _convert_date(self, date_str):
        """
        è½‰æ›æ—¥æœŸæ ¼å¼
        
        Args:
            date_str (str): åŸå§‹æ—¥æœŸå­—ç¬¦ä¸²
            
        Returns:
            str: æ¨™æº–æ ¼å¼æ—¥æœŸ
        """
        # è™•ç†æ°‘åœ‹å¹´æ ¼å¼ (114.07.30 æˆ– 114å¹´07æœˆ30æ—¥)
        if 'å¹´' in date_str and 'æœˆ' in date_str:
            # 114å¹´07æœˆ30æ—¥ æ ¼å¼
            pattern = r'(\d{2,3})å¹´(\d{1,2})æœˆ(\d{1,2})æ—¥'
            match = re.match(pattern, date_str)
            if match:
                year, month, day = match.groups()
                year = int(year) + 1911  # æ°‘åœ‹å¹´è½‰è¥¿å…ƒå¹´
                return f"{year}-{int(month):02d}-{int(day):02d}"
        
        elif '.' in date_str:
            # 114.07.30 æ ¼å¼
            parts = date_str.split('.')
            if len(parts) == 3:
                year, month, day = parts
                year = int(year) + 1911  # æ°‘åœ‹å¹´è½‰è¥¿å…ƒå¹´
                return f"{year}-{int(month):02d}-{int(day):02d}"
        
        return date_str  # å¦‚æœç„¡æ³•è§£æï¼Œè¿”å›åŸå§‹å­—ç¬¦ä¸²
    
    def load_history(self):
        """è¼‰å…¥æ­·å²è¨˜éŒ„"""
        history_file = self.data_dir / "smart_history.json"
        
        if history_file.exists():
            try:
                with open(history_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except Exception as e:
                print(f"âš ï¸ è¼‰å…¥æ­·å²è¨˜éŒ„å¤±æ•—: {e}")
                return []
        return []
    
    def save_history(self, all_rulings):
        """å„²å­˜æ­·å²è¨˜éŒ„"""
        history_file = self.data_dir / "smart_history.json"
        
        try:
            with open(history_file, 'w', encoding='utf-8') as f:
                json.dump(all_rulings, f, ensure_ascii=False, indent=2)
            print(f"âœ… æ­·å²è¨˜éŒ„å·²å„²å­˜: {history_file}")
        except Exception as e:
            print(f"âŒ å„²å­˜æ­·å²è¨˜éŒ„å¤±æ•—: {e}")
    
    def compare_and_update(self, new_rulings):
        """æ¯”å°æ–°èˆŠè³‡æ–™ä¸¦æ›´æ–°"""
        history = self.load_history()
        
        # å»ºç«‹æ­·å²è³‡æ–™çš„æ¨™é¡Œé›†åˆ
        existing_titles = {ruling.get('title', '') for ruling in history}
        
        # æ‰¾å‡ºæ–°çš„å‡½é‡‹
        new_items = []
        for ruling in new_rulings:
            if ruling.get('title', '') not in existing_titles:
                new_items.append(ruling)
        
        # æ›´æ–°æ­·å²è¨˜éŒ„
        updated_history = history + new_items
        
        # æŒ‰æ—¥æœŸæ’åº
        updated_history.sort(key=lambda x: x.get('date', ''), reverse=True)
        
        # å„²å­˜æ›´æ–°å¾Œçš„æ­·å²
        self.save_history(updated_history)
        
        return new_items, updated_history
    
    def generate_report(self, new_rulings, total_rulings):
        """ç”ŸæˆåŸ·è¡Œå ±å‘Š"""
        report = {
            'execution_time': datetime.now(self.tz_taipei).strftime('%Y-%m-%d %H:%M:%S'),
            'total_checked': len(total_rulings),
            'new_count': len(new_rulings),
            'has_new': len(new_rulings) > 0,
            'source': 'MOF_Taiwan_Fixed',
            'scraper_version': '1.1_url_fixed',
            'url_statistics': self._analyze_url_status(total_rulings)
        }
        
        # å„²å­˜ä»Šæ—¥å ±å‘Š
        report_file = self.data_dir / "daily_report.json"
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)
        
        # å„²å­˜æ–°ç™¼ç¾çš„å‡½é‡‹
        if new_rulings:
            new_file = self.data_dir / "today_new.json"
            with open(new_file, 'w', encoding='utf-8') as f:
                json.dump(new_rulings, f, ensure_ascii=False, indent=2)
        
        return report
    
    def _analyze_url_status(self, rulings):
        """åˆ†æ URL ç‹€æ…‹çµ±è¨ˆ"""
        stats = {
            'total': len(rulings),
            'valid_urls': 0,
            'invalid_urls': 0,
            'missing_urls': 0
        }
        
        for ruling in rulings:
            status = ruling.get('url_status', 'unknown')
            if status == 'valid':
                stats['valid_urls'] += 1
            elif status == 'invalid':
                stats['invalid_urls'] += 1
            elif status == 'missing':
                stats['missing_urls'] += 1
        
        return stats

def main():
    """ä¸»ç¨‹å¼"""
    print("="*50)
    print("ğŸ’¼ ä¿®å¾©ç‰ˆè²¡æ”¿éƒ¨ç¨…å‹™å‡½é‡‹çˆ¬èŸ²")
    print(f"ğŸ• åŸ·è¡Œæ™‚é–“: {datetime.now()}")
    print("ğŸ”§ ä¿®å¾©é …ç›®: URL é€£çµå•é¡Œ")
    print("="*50)
    
    scraper = FixedTaxScraper()
    
    print("\nğŸ” çˆ¬å–æœ€æ–°å‡½é‡‹...")
    new_rulings = scraper.fetch_latest_rulings()
    
    if not new_rulings:
        print("âŒ æœªèƒ½ç²å–ä»»ä½•å‡½é‡‹è³‡æ–™")
        return
    
    print(f"âœ… çˆ¬å–å®Œæˆï¼Œå…±ç²å¾— {len(new_rulings)} ç­†è³‡æ–™")
    
    # æ¯”å°ä¸¦æ›´æ–°æ­·å²è¨˜éŒ„
    print("\nğŸ“Š æ¯”å°æ­·å²è³‡æ–™...")
    new_items, all_rulings = scraper.compare_and_update(new_rulings)
    
    if new_items:
        print(f"ğŸ†• ç™¼ç¾ {len(new_items)} ç­†æ–°å‡½é‡‹ï¼")
        for i, item in enumerate(new_items, 1):
            print(f"   {i}. {item.get('title', 'ç„¡æ¨™é¡Œ')[:50]}...")
    else:
        print("âœ¨ ä»Šå¤©æ²’æœ‰æ–°å‡½é‡‹")
    
    # ç”Ÿæˆå ±å‘Š
    print("\nğŸ“‹ ç”ŸæˆåŸ·è¡Œå ±å‘Š...")
    report = scraper.generate_report(new_items, new_rulings)
    
    # é¡¯ç¤º URL ä¿®å¾©çµ±è¨ˆ
    url_stats = report['url_statistics']
    print(f"\nğŸ”— URL ä¿®å¾©çµ±è¨ˆ:")
    print(f"   â€¢ ç¸½è¨ˆ: {url_stats['total']} ç­†")
    print(f"   â€¢ æœ‰æ•ˆé€£çµ: {url_stats['valid_urls']} ç­†")
    print(f"   â€¢ ç„¡æ•ˆé€£çµ: {url_stats['invalid_urls']} ç­†")
    print(f"   â€¢ ç¼ºå°‘é€£çµ: {url_stats['missing_urls']} ç­†")
    
    print(f"\nâœ… åŸ·è¡Œå®Œæˆï¼")

if __name__ == "__main__":
    main()
