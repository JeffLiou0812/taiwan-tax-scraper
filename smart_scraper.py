#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
è²¡æ”¿éƒ¨è³¦ç¨…ç½²æ³•è¦æŸ¥è©¢ç³»çµ± - æ–°é ’å‡½é‡‹çˆ¬èŸ²ï¼ˆæœ€çµ‚é˜²éŒ¯ç‰ˆï¼‰
å®Œæ•´æ•´åˆæ‰€æœ‰éå»éŒ¯èª¤çš„é˜²è­·æªæ–½

éŒ¯èª¤é˜²è­·æ¸…å–®ï¼š
âœ… 1. GitHub Actions æ¬Šé™å•é¡Œ - é€éæ­£ç¢ºçš„æª”æ¡ˆè¼¸å‡ºæ ¼å¼è§£æ±º
âœ… 2. URLæ ¼å¼éŒ¯èª¤ - å¤šé‡é©—è­‰å’Œä¿®å¾©æ©Ÿåˆ¶
âœ… 3. æ—¥æœŸè™•ç† - ä¿ç•™åŸå§‹æ ¼å¼ï¼Œä¸é€²è¡Œè½‰æ›
âœ… 4. YAMLèªæ³• - ç¢ºä¿è¼¸å‡ºJSONæ ¼å¼æ­£ç¢º
âœ… 5. å…ƒç´ å®šä½å¤±æ•— - å¤šé‡è§£æç­–ç•¥
âœ… 6. è·¯å¾‘å•é¡Œ - ä½¿ç”¨Pathç‰©ä»¶è™•ç†
âœ… 7. robots.txt - å®Œæ•´çš„è«‹æ±‚æ¨™é ­
âœ… 8. é‡è©¦æ©Ÿåˆ¶ - æŒ‡æ•¸é€€é¿ç­–ç•¥

ç›®æ¨™ç¶²ç«™: https://law.dot.gov.tw/law-ch/home.jsp
ç‰ˆæœ¬: 6.0 Final Protected
æ›´æ–°æ—¥æœŸ: 2025-08-20
"""

import requests
import json
import pandas as pd
from datetime import datetime, timezone, timedelta
import re
from pathlib import Path
import hashlib
import time
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse, urlencode
import logging
from typing import Dict, List, Tuple, Optional
import traceback

class UltimateProtectedScraper:
    """æœ€çµ‚é˜²éŒ¯ç‰ˆçˆ¬èŸ² - å®Œæ•´éŒ¯èª¤é˜²è­·"""
    
    def __init__(self, data_dir="data", debug=True):
        """
        åˆå§‹åŒ–çˆ¬èŸ²
        éŒ¯èª¤é˜²è­·6ï¼šä½¿ç”¨Pathç‰©ä»¶è™•ç†æ‰€æœ‰è·¯å¾‘ï¼Œé¿å…Windows/Linuxå·®ç•°
        """
        # ä½¿ç”¨Pathç‰©ä»¶è™•ç†è·¯å¾‘ï¼ˆé¿å…è·¯å¾‘éŒ¯èª¤ï¼‰
        self.data_dir = Path(data_dir)
        self.data_dir.mkdir(exist_ok=True)
        
        # è¨­å®šæ—¥èªŒç³»çµ±
        self.setup_logging(debug)
        
        # ç¶²ç«™è¨­å®š
        self.base_url = "https://law.dot.gov.tw"
        self.search_url = "https://law.dot.gov.tw/law-ch/home.jsp"
        
        # æŸ¥è©¢åƒæ•¸ - é€™äº›åƒæ•¸ç¢ºä¿æˆ‘å€‘ç²å–æ–°é ’å‡½é‡‹
        self.search_params = {
            'id': '18',
            'contentid': '18',
            'parentpath': '0,7',
            'mcustomize': 'newlaw_list.jsp',
            'istype': 'L',
            'classtablename': 'LawClass',
            'sort': '1',
            'up_down': 'D'  # é™åºæ’åˆ—ï¼Œæœ€æ–°çš„åœ¨å‰
        }
        
        # å°ç£æ™‚å€
        self.tz_taipei = timezone(timedelta(hours=8))
        
        # éŒ¯èª¤é˜²è­·7ï¼šå®Œæ•´çš„è«‹æ±‚æ¨™é ­ï¼Œé¿å…è¢«robots.txté˜»æ“‹
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8',
            'Accept-Language': 'zh-TW,zh;q=0.9,en;q=0.8,zh-CN;q=0.7',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Cache-Control': 'max-age=0',
            'Upgrade-Insecure-Requests': '1',
            'Sec-Fetch-Dest': 'document',
            'Sec-Fetch-Mode': 'navigate',
            'Sec-Fetch-Site': 'none',
            'Sec-Fetch-User': '?1',
            'Referer': 'https://law.dot.gov.tw/',
            'DNT': '1'
        }
        
        # å»ºç«‹Session
        self.session = requests.Session()
        self.session.headers.update(self.headers)
        
        # éŒ¯èª¤çµ±è¨ˆ
        self.error_stats = {
            'url_errors_fixed': 0,
            'parse_errors_recovered': 0,
            'retry_attempts': 0,
            'total_errors': 0
        }
    
    def setup_logging(self, debug: bool):
        """è¨­å®šæ—¥èªŒç³»çµ±"""
        log_level = logging.DEBUG if debug else logging.INFO
        
        # åŒæ™‚è¼¸å‡ºåˆ°æª”æ¡ˆå’Œæ§åˆ¶å°
        log_format = '%(asctime)s - %(levelname)s - %(message)s'
        logging.basicConfig(
            level=log_level,
            format=log_format,
            datefmt='%Y-%m-%d %H:%M:%S'
        )
        
        self.logger = logging.getLogger(__name__)
        
        # é¡å¤–å„²å­˜éŒ¯èª¤æ—¥èªŒåˆ°æª”æ¡ˆ
        error_log = self.data_dir / 'error_log.txt'
        if debug:
            fh = logging.FileHandler(error_log, encoding='utf-8')
            fh.setLevel(logging.ERROR)
            fh.setFormatter(logging.Formatter(log_format))
            self.logger.addHandler(fh)
    
    def safe_request(self, url: str, params: Dict = None, max_retries: int = 3) -> Optional[requests.Response]:
        """
        éŒ¯èª¤é˜²è­·8ï¼šå®‰å…¨çš„ç¶²è·¯è«‹æ±‚ï¼ŒåŒ…å«é‡è©¦æ©Ÿåˆ¶å’ŒæŒ‡æ•¸é€€é¿
        é€™æ˜¯åŸºæ–¼éå»ç¶²è·¯éŒ¯èª¤çš„ç¶“é©—è¨­è¨ˆçš„
        """
        for attempt in range(max_retries):
            try:
                self.logger.debug(f"è«‹æ±‚å˜—è©¦ {attempt + 1}/{max_retries}: {url}")
                
                # ç™¼é€è«‹æ±‚
                response = self.session.get(
                    url,
                    params=params,
                    timeout=30,
                    verify=True,
                    allow_redirects=True
                )
                
                # æª¢æŸ¥ç‹€æ…‹ç¢¼
                if response.status_code == 200:
                    self.logger.debug("è«‹æ±‚æˆåŠŸ")
                    return response
                elif response.status_code == 404:
                    self.logger.error(f"é é¢ä¸å­˜åœ¨ (404): {url}")
                    return None
                else:
                    self.logger.warning(f"HTTP {response.status_code}")
                    self.error_stats['retry_attempts'] += 1
                    
            except requests.exceptions.Timeout:
                self.logger.error(f"è«‹æ±‚è¶…æ™‚ (å˜—è©¦ {attempt + 1}/{max_retries})")
                self.error_stats['total_errors'] += 1
            except requests.exceptions.ConnectionError:
                self.logger.error(f"é€£ç·šéŒ¯èª¤ (å˜—è©¦ {attempt + 1}/{max_retries})")
                self.error_stats['total_errors'] += 1
            except Exception as e:
                self.logger.error(f"æœªé æœŸçš„éŒ¯èª¤: {e}")
                self.error_stats['total_errors'] += 1
            
            # æŒ‡æ•¸é€€é¿ç­–ç•¥
            if attempt < max_retries - 1:
                wait_time = min(2 ** attempt, 10)  # æœ€å¤šç­‰å¾…10ç§’
                self.logger.info(f"ç­‰å¾… {wait_time} ç§’å¾Œé‡è©¦...")
                time.sleep(wait_time)
        
        self.logger.error(f"æ‰€æœ‰é‡è©¦éƒ½å¤±æ•—: {url}")
        return None
    
    def fix_url_comprehensive(self, url: str) -> str:
        """
        éŒ¯èª¤é˜²è­·2ï¼šå…¨é¢çš„URLä¿®å¾©æ©Ÿåˆ¶
        åŸºæ–¼éå» law.dot.gov.twhome.jsp éŒ¯èª¤çš„å®Œæ•´è§£æ±ºæ–¹æ¡ˆ
        """
        if not url:
            return ""
        
        original_url = url
        url = str(url).strip()
        
        # æ ¸å¿ƒé˜²è­·ï¼šæª¢æ¸¬ä¸¦ä¿®å¾©å·²çŸ¥çš„éŒ¯èª¤æ¨¡å¼
        error_patterns = [
            ('twhome.jsp', '/home.jsp'),
            ('gov.twhome', 'gov.tw/home'),
            ('lawlaw', 'law'),  # é¿å…é‡è¤‡
            ('//', '/'),  # é¿å…é›™æ–œç·šï¼ˆé™¤äº†https://ï¼‰
        ]
        
        for error_pattern, correct_pattern in error_patterns:
            if error_pattern in url and error_pattern != '//':
                self.logger.warning(f"åµæ¸¬åˆ°URLéŒ¯èª¤æ¨¡å¼: {error_pattern}")
                url = url.replace(error_pattern, correct_pattern)
                self.error_stats['url_errors_fixed'] += 1
        
        # è™•ç†é›™æ–œç·šï¼ˆä¿ç•™https://ï¼‰
        if '//' in url and not url.startswith('http'):
            url = re.sub(r'(?<!:)//', '/', url)
        
        # ç¢ºä¿URLå®Œæ•´æ€§
        if not url.startswith(('http://', 'https://')):
            if url.startswith('/'):
                # çµ•å°è·¯å¾‘
                url = self.base_url + url
            else:
                # ç›¸å°è·¯å¾‘
                url = urljoin(f"{self.base_url}/law-ch/", url)
        
        # å¼·åˆ¶HTTPSï¼ˆæ”¿åºœç¶²ç«™æ‡‰è©²éƒ½æ”¯æ´ï¼‰
        if url.startswith('http://law.dot.gov.tw'):
            url = url.replace('http://', 'https://')
        
        # æœ€çµ‚é©—è­‰
        try:
            result = urlparse(url)
            if not all([result.scheme, result.netloc]):
                self.logger.error(f"URLé©—è­‰å¤±æ•—: {url}")
                return original_url  # è¿”å›åŸå§‹URLä½œç‚ºå‚™æ¡ˆ
        except:
            return original_url
        
        if url != original_url:
            self.logger.info(f"URLå·²ä¿®å¾©: {original_url} -> {url}")
        
        return url
    
    def extract_date(self, text: str) -> str:
        """
        éŒ¯èª¤é˜²è­·3ï¼ˆç°¡åŒ–ç‰ˆï¼‰ï¼šæå–æ—¥æœŸä½†ä¸è½‰æ›
        æ ¹æ“šæ‚¨çš„è¦æ±‚ï¼Œä¿ç•™åŸå§‹æ°‘åœ‹å¹´æ ¼å¼
        """
        if not text:
            return ""
        
        # æœå°‹å„ç¨®æ—¥æœŸæ ¼å¼
        date_patterns = [
            r'\d{2,3}å¹´\d{1,2}æœˆ\d{1,2}æ—¥',
            r'\d{2,3}\.\d{1,2}\.\d{1,2}',
            r'\d{2,3}/\d{1,2}/\d{1,2}',
            r'æ°‘åœ‹\d{2,3}å¹´\d{1,2}æœˆ\d{1,2}æ—¥',
        ]
        
        for pattern in date_patterns:
            match = re.search(pattern, text)
            if match:
                return match.group(0)
        
        return ""
    
    def parse_rulings_smart(self, html_content: str) -> List[Dict]:
        """
        éŒ¯èª¤é˜²è­·5ï¼šæ™ºèƒ½è§£æï¼Œä½¿ç”¨å¤šé‡ç­–ç•¥é¿å…å…ƒç´ å®šä½å¤±æ•—
        é€™æ˜¯åŸºæ–¼éå»Seleniumå®šä½å¤±æ•—çš„ç¶“é©—è¨­è¨ˆçš„
        """
        rulings = []
        
        try:
            soup = BeautifulSoup(html_content, 'html.parser')
            
            # ç­–ç•¥1ï¼šè¡¨æ ¼è§£æï¼ˆæœ€å¸¸è¦‹ï¼‰
            self.logger.debug("å˜—è©¦è¡¨æ ¼è§£æç­–ç•¥...")
            tables = soup.find_all('table')
            
            for table in tables:
                # è·³éå°èˆªè¡¨æ ¼
                if 'navigation' in str(table.get('class', [])).lower():
                    continue
                
                rows = table.find_all('tr')
                for row in rows[1:]:  # è·³éæ¨™é¡Œåˆ—
                    cells = row.find_all(['td', 'th'])
                    if len(cells) >= 2:
                        ruling = self.extract_ruling_from_cells(cells)
                        if ruling:
                            rulings.append(ruling)
            
            # ç­–ç•¥2ï¼šå¦‚æœè¡¨æ ¼è§£æå¤±æ•—ï¼Œå˜—è©¦div/åˆ—è¡¨è§£æ
            if not rulings:
                self.logger.debug("è¡¨æ ¼è§£æç„¡çµæœï¼Œå˜—è©¦åˆ—è¡¨è§£æ...")
                
                # å°‹æ‰¾å¯èƒ½åŒ…å«å‡½é‡‹çš„å®¹å™¨
                containers = soup.find_all(['div', 'ul', 'ol'], class_=re.compile(r'law|list|item|content'))
                
                for container in containers:
                    items = container.find_all(['li', 'div', 'p'])
                    for item in items:
                        ruling = self.extract_ruling_from_element(item)
                        if ruling:
                            rulings.append(ruling)
            
            # ç­–ç•¥3ï¼šæœ€å¾Œçš„å‚™æ¡ˆ - å…¨æ–‡æœå°‹
            if not rulings:
                self.logger.debug("åˆ—è¡¨è§£æç„¡çµæœï¼Œä½¿ç”¨å…¨æ–‡æœå°‹...")
                
                # æœå°‹æ‰€æœ‰åŒ…å«æ—¥æœŸçš„æ®µè½
                all_text = soup.get_text()
                lines = all_text.split('\n')
                
                for i, line in enumerate(lines):
                    if self.extract_date(line):
                        # æ‰¾åˆ°æ—¥æœŸï¼Œå˜—è©¦æå–ç›¸é—œè³‡è¨Š
                        ruling = {
                            'date': self.extract_date(line),
                            'title': lines[i+1] if i+1 < len(lines) else line,
                            'source': 'DOT_Taiwan',
                            'scrape_time': datetime.now(self.tz_taipei).isoformat()
                        }
                        ruling['id'] = self.generate_id(ruling)
                        rulings.append(ruling)
            
            self.logger.info(f"æˆåŠŸè§£æ {len(rulings)} ç­†å‡½é‡‹")
            
        except Exception as e:
            self.logger.error(f"è§£æéŒ¯èª¤: {e}")
            self.error_stats['total_errors'] += 1
            # éŒ¯èª¤æ¢å¾©ï¼šå³ä½¿è§£æå¤±æ•—ä¹Ÿè¿”å›ç©ºåˆ—è¡¨è€Œä¸æ˜¯å´©æ½°
            self.error_stats['parse_errors_recovered'] += 1
        
        return rulings
    
    def extract_ruling_from_cells(self, cells) -> Optional[Dict]:
        """å¾è¡¨æ ¼å„²å­˜æ ¼æå–å‡½é‡‹è³‡è¨Š"""
        try:
            ruling = {
                'source': 'DOT_Taiwan',
                'scrape_time': datetime.now(self.tz_taipei).isoformat()
            }
            
            has_content = False
            
            for cell in cells:
                cell_text = cell.get_text(strip=True)
                
                # æå–æ—¥æœŸï¼ˆä¸è½‰æ›ï¼‰
                date = self.extract_date(cell_text)
                if date and 'date' not in ruling:
                    ruling['date'] = date
                    has_content = True
                
                # æå–å­—è™Ÿ
                if re.search(r'[å°è²¡ç¨…].*?ç¬¬?\d+è™Ÿ', cell_text):
                    ruling['doc_number'] = cell_text
                    has_content = True
                
                # æå–é€£çµå’Œæ¨™é¡Œ
                link = cell.find('a')
                if link:
                    ruling['title'] = link.get_text(strip=True)
                    href = link.get('href', '')
                    if href:
                        ruling['url'] = self.fix_url_comprehensive(href)
                        ruling['original_url'] = href  # ä¿ç•™åŸå§‹URLä¾›é™¤éŒ¯
                    has_content = True
                elif len(cell_text) > 10 and 'title' not in ruling:
                    ruling['title'] = cell_text[:200]
                    has_content = True
            
            if has_content:
                ruling['id'] = self.generate_id(ruling)
                return ruling
                
        except Exception as e:
            self.logger.debug(f"å„²å­˜æ ¼æå–éŒ¯èª¤: {e}")
            
        return None
    
    def extract_ruling_from_element(self, element) -> Optional[Dict]:
        """å¾HTMLå…ƒç´ æå–å‡½é‡‹è³‡è¨Š"""
        try:
            text = element.get_text(strip=True)
            
            if len(text) < 10:  # å¤ªçŸ­çš„æ–‡å­—å¿½ç•¥
                return None
            
            ruling = {
                'source': 'DOT_Taiwan',
                'scrape_time': datetime.now(self.tz_taipei).isoformat()
            }
            
            # æå–æ—¥æœŸ
            date = self.extract_date(text)
            if date:
                ruling['date'] = date
            
            # æå–å­—è™Ÿ
            doc_match = re.search(r'([å°è²¡ç¨…].*?ç¬¬?\d+è™Ÿ)', text)
            if doc_match:
                ruling['doc_number'] = doc_match.group(1)
            
            # æå–é€£çµ
            link = element.find('a')
            if link:
                ruling['title'] = link.get_text(strip=True)
                href = link.get('href', '')
                if href:
                    ruling['url'] = self.fix_url_comprehensive(href)
            else:
                ruling['title'] = text[:200]
            
            # åªæœ‰åœ¨æœ‰å¯¦è³ªå…§å®¹æ™‚æ‰è¿”å›
            if ruling.get('date') or ruling.get('title'):
                ruling['id'] = self.generate_id(ruling)
                return ruling
                
        except Exception as e:
            self.logger.debug(f"å…ƒç´ æå–éŒ¯èª¤: {e}")
            
        return None
    
    def generate_id(self, ruling: Dict) -> str:
        """ç”Ÿæˆå”¯ä¸€è­˜åˆ¥ç¢¼"""
        key_parts = [
            ruling.get('date', ''),
            ruling.get('doc_number', ''),
            ruling.get('title', '')[:50]
        ]
        key_string = '|'.join(filter(None, key_parts))
        return hashlib.md5(key_string.encode('utf-8')).hexdigest()[:12]
    
    def fetch_new_rulings(self, max_pages: int = 3) -> List[Dict]:
        """
        ä¸»è¦çˆ¬å–å‡½æ•¸ - åŒ…å«æ‰€æœ‰éŒ¯èª¤é˜²è­·æ©Ÿåˆ¶
        """
        all_rulings = []
        
        self.logger.info("="*60)
        self.logger.info("é–‹å§‹çˆ¬å–è²¡æ”¿éƒ¨è³¦ç¨…ç½²æ–°é ’å‡½é‡‹")
        self.logger.info(f"ç›®æ¨™: {self.search_url}")
        self.logger.info("="*60)
        
        for page in range(1, max_pages + 1):
            try:
                # æº–å‚™åƒæ•¸
                params = self.search_params.copy()
                if page > 1:
                    params['page'] = str(page)
                
                self.logger.info(f"\næ­£åœ¨çˆ¬å–ç¬¬ {page} é ...")
                
                # å®‰å…¨è«‹æ±‚
                response = self.safe_request(self.search_url, params)
                
                if not response:
                    self.logger.warning(f"ç¬¬ {page} é ç„¡æ³•å–å¾—")
                    if page == 1:
                        # ç¬¬ä¸€é å°±å¤±æ•—ï¼Œé€™æ˜¯åš´é‡å•é¡Œ
                        self.logger.error("ç„¡æ³•å–å¾—ç¬¬ä¸€é è³‡æ–™ï¼Œåœæ­¢çˆ¬å–")
                        break
                    continue
                
                # æ™ºèƒ½è§£æ
                page_rulings = self.parse_rulings_smart(response.text)
                
                if not page_rulings:
                    self.logger.info(f"ç¬¬ {page} é ç„¡è³‡æ–™ï¼Œåœæ­¢çˆ¬å–")
                    break
                
                all_rulings.extend(page_rulings)
                self.logger.info(f"ç¬¬ {page} é æˆåŠŸ: {len(page_rulings)} ç­†")
                
                # é¿å…éå¿«è«‹æ±‚
                if page < max_pages:
                    time.sleep(1)
                    
            except Exception as e:
                self.logger.error(f"çˆ¬å–ç¬¬ {page} é ç™¼ç”ŸéŒ¯èª¤: {e}")
                self.error_stats['total_errors'] += 1
                # ç¹¼çºŒä¸‹ä¸€é è€Œä¸æ˜¯å®Œå…¨åœæ­¢
                continue
        
        self.logger.info(f"\nç¸½å…±çˆ¬å–: {len(all_rulings)} ç­†å‡½é‡‹")
        return all_rulings
    
    def compare_and_update(self, new_rulings: List[Dict]) -> Tuple[List[Dict], List[Dict]]:
        """
        éŒ¯èª¤é˜²è­·4ï¼šå®‰å…¨çš„æ­·å²è¨˜éŒ„æ¯”å°
        ç¢ºä¿JSONè®€å¯«ä¸æœƒé€ æˆYAMLèªæ³•å•é¡Œ
        """
        history_file = self.data_dir / "smart_history.json"
        
        # å®‰å…¨è®€å–æ­·å²è¨˜éŒ„
        history = []
        if history_file.exists():
            try:
                with open(history_file, 'r', encoding='utf-8') as f:
                    content = f.read()
                    if content:  # ç¢ºä¿æª”æ¡ˆä¸æ˜¯ç©ºçš„
                        history = json.loads(content)
                    self.logger.info(f"è¼‰å…¥ {len(history)} ç­†æ­·å²è¨˜éŒ„")
            except json.JSONDecodeError as e:
                self.logger.error(f"JSONè§£æéŒ¯èª¤: {e}")
                # å‚™ä»½æå£çš„æª”æ¡ˆ
                backup_file = history_file.with_suffix('.json.backup')
                history_file.rename(backup_file)
                self.logger.info(f"å·²å‚™ä»½æå£çš„æ­·å²æª”æ¡ˆåˆ° {backup_file}")
            except Exception as e:
                self.logger.error(f"è®€å–æ­·å²è¨˜éŒ„å¤±æ•—: {e}")
        
        # æ¯”å°æ–°è³‡æ–™
        history_ids = {item.get('id') for item in history if item.get('id')}
        new_items = []
        
        for ruling in new_rulings:
            if ruling.get('id') and ruling['id'] not in history_ids:
                new_items.append(ruling)
        
        self.logger.info(f"ç™¼ç¾ {len(new_items)} ç­†æ–°å‡½é‡‹")
        
        # å®‰å…¨æ›´æ–°æ­·å²è¨˜éŒ„
        if new_items:
            history.extend(new_items)
            
            # é™åˆ¶å¤§å°
            if len(history) > 1000:
                history = history[-1000:]
            
            # å®‰å…¨å¯«å…¥
            try:
                # å…ˆå¯«å…¥æš«å­˜æª”
                temp_file = history_file.with_suffix('.json.tmp')
                with open(temp_file, 'w', encoding='utf-8') as f:
                    json.dump(history, f, ensure_ascii=False, indent=2)
                
                # é©—è­‰æš«å­˜æª”
                with open(temp_file, 'r', encoding='utf-8') as f:
                    json.load(f)  # æ¸¬è©¦æ˜¯å¦èƒ½æ­£ç¢ºè®€å–
                
                # æ›¿æ›åŸæª”æ¡ˆ
                temp_file.replace(history_file)
                self.logger.info("æ­·å²è¨˜éŒ„å·²å®‰å…¨æ›´æ–°")
                
            except Exception as e:
                self.logger.error(f"æ›´æ–°æ­·å²è¨˜éŒ„å¤±æ•—: {e}")
                if temp_file.exists():
                    temp_file.unlink()  # åˆªé™¤æš«å­˜æª”
        
        return new_items, history
    
    def generate_report(self, new_items: List[Dict], total_rulings: List[Dict]) -> Dict:
        """
        éŒ¯èª¤é˜²è­·1ï¼šç”Ÿæˆæ­£ç¢ºæ ¼å¼çš„å ±å‘Šä¾›GitHub Actionsè®€å–
        é€™å€‹å ±å‘Šæ ¼å¼ç¶“éç²¾å¿ƒè¨­è¨ˆï¼Œç¢ºä¿å·¥ä½œæµç¨‹èƒ½æ­£ç¢ºè®€å–
        """
        report = {
            'execution_time': datetime.now(self.tz_taipei).isoformat(),
            'execution_date': datetime.now(self.tz_taipei).strftime('%Y-%m-%d'),
            'total_checked': len(total_rulings),
            'new_count': len(new_items),
            'has_new': len(new_items) > 0,  # å¸ƒæ—å€¼ï¼Œå·¥ä½œæµç¨‹ç”¨é€™å€‹åˆ¤æ–·æ˜¯å¦é€šçŸ¥
            'source': 'law.dot.gov.tw',
            'scraper_version': '6.0_Final_Protected',
            'error_statistics': self.error_stats,
            'status': 'success' if total_rulings else 'no_data'
        }
        
        # ç¢ºä¿å ±å‘Šæª”æ¡ˆæ­£ç¢ºå¯«å…¥
        report_file = self.data_dir / "daily_report.json"
        try:
            with open(report_file, 'w', encoding='utf-8') as f:
                json.dump(report, f, ensure_ascii=False, indent=2)
            self.logger.info("å ±å‘Šå·²ç”Ÿæˆ")
        except Exception as e:
            self.logger.error(f"å ±å‘Šç”Ÿæˆå¤±æ•—: {e}")
            # å³ä½¿å¤±æ•—ä¹Ÿè¦å»ºç«‹åŸºæœ¬å ±å‘Š
            basic_report = {'has_new': False, 'status': 'error'}
            with open(report_file, 'w') as f:
                json.dump(basic_report, f)
        
        # å„²å­˜æ–°å‡½é‡‹ä¾›é€šçŸ¥ä½¿ç”¨
        if new_items:
            new_file = self.data_dir / "today_new.json"
            try:
                with open(new_file, 'w', encoding='utf-8') as f:
                    json.dump(new_items, f, ensure_ascii=False, indent=2)
            except Exception as e:
                self.logger.error(f"å„²å­˜æ–°å‡½é‡‹å¤±æ•—: {e}")
        
        return report
    
    def save_results(self, rulings: List[Dict]) -> None:
        """å„²å­˜çµæœç‚ºå¤šç¨®æ ¼å¼"""
        if not rulings:
            return
        
        timestamp = datetime.now(self.tz_taipei).strftime('%Y%m%d_%H%M%S')
        
        # JSONæ ¼å¼
        json_file = self.data_dir / f'smart_results_{timestamp}.json'
        try:
            with open(json_file, 'w', encoding='utf-8') as f:
                json.dump(rulings, f, ensure_ascii=False, indent=2)
            self.logger.info(f"JSONå·²å„²å­˜: {json_file.name}")
        except Exception as e:
            self.logger.error(f"JSONå„²å­˜å¤±æ•—: {e}")
        
        # CSVæ ¼å¼
        csv_file = self.data_dir / f'smart_results_{timestamp}.csv'
        try:
            df = pd.DataFrame(rulings)
            df.to_csv(csv_file, index=False, encoding='utf-8-sig')
            self.logger.info(f"CSVå·²å„²å­˜: {csv_file.name}")
        except Exception as e:
            self.logger.error(f"CSVå„²å­˜å¤±æ•—: {e}")

def main():
    """
    ä¸»ç¨‹å¼ - åŒ…å«å®Œæ•´çš„éŒ¯èª¤è™•ç†å’Œæ¢å¾©æ©Ÿåˆ¶
    """
    print("="*70)
    print("ğŸ›ï¸ è²¡æ”¿éƒ¨è³¦ç¨…ç½²æ–°é ’å‡½é‡‹çˆ¬èŸ² - æœ€çµ‚é˜²éŒ¯ç‰ˆ")
    print(f"ğŸ“ ç›®æ¨™ç¶²ç«™: law.dot.gov.tw")
    print(f"ğŸ• åŸ·è¡Œæ™‚é–“: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"ğŸ›¡ï¸ éŒ¯èª¤é˜²è­·: å…¨éƒ¨å•Ÿç”¨")
    print("="*70)
    
    # å…¨åŸŸéŒ¯èª¤æ•æ‰
    try:
        # åˆå§‹åŒ–
        print("\nâš™ï¸ åˆå§‹åŒ–ç³»çµ±...")
        scraper = UltimateProtectedScraper(debug=True)
        
        # çˆ¬å–
        print("\nğŸ“¡ é–‹å§‹çˆ¬å–...")
        rulings = scraper.fetch_new_rulings(max_pages=3)
        
        if not rulings:
            print("\nâš ï¸ æœªçˆ¬å–åˆ°è³‡æ–™")
            # å³ä½¿æ²’è³‡æ–™ä¹Ÿè¦ç”Ÿæˆå ±å‘Š
            scraper.generate_report([], [])
            return
        
        print(f"\nâœ… æˆåŠŸçˆ¬å– {len(rulings)} ç­†å‡½é‡‹")
        
        # è³‡æ–™é è¦½
        print("\nğŸ“‹ è³‡æ–™é è¦½ï¼š")
        for i, ruling in enumerate(rulings[:3], 1):
            print(f"  {i}. {ruling.get('title', 'N/A')[:50]}")
            print(f"     æ—¥æœŸ: {ruling.get('date', 'N/A')}")
        
        # æ¯”å°æ­·å²
        print("\nğŸ” æ¯”å°æ­·å²è¨˜éŒ„...")
        new_items, history = scraper.compare_and_update(rulings)
        
        if new_items:
            print(f"\nğŸ‰ ç™¼ç¾ {len(new_items)} ç­†æ–°å‡½é‡‹")
        else:
            print("\nâœ¨ ç„¡æ–°å‡½é‡‹")
        
        # å„²å­˜
        print("\nğŸ’¾ å„²å­˜è³‡æ–™...")
        scraper.save_results(rulings)
        
        # å ±å‘Š
        print("\nğŸ“Š ç”Ÿæˆå ±å‘Š...")
        report = scraper.generate_report(new_items, rulings)
        
        # çµ±è¨ˆ
        print("\nğŸ“ˆ åŸ·è¡Œçµ±è¨ˆï¼š")
        print(f"  â€¢ URLä¿®å¾©: {report['error_statistics']['url_errors_fixed']}")
        print(f"  â€¢ éŒ¯èª¤æ¢å¾©: {report['error_statistics']['parse_errors_recovered']}")
        print(f"  â€¢ é‡è©¦æ¬¡æ•¸: {report['error_statistics']['retry_attempts']}")
        print(f"  â€¢ ç¸½éŒ¯èª¤æ•¸: {report['error_statistics']['total_errors']}")
        
        print("\nâœ… åŸ·è¡Œå®Œæˆï¼")
        
    except KeyboardInterrupt:
        print("\nâš ï¸ ä½¿ç”¨è€…ä¸­æ–·åŸ·è¡Œ")
    except Exception as e:
        print(f"\nâŒ åš´é‡éŒ¯èª¤: {e}")
        traceback.print_exc()
        
        # ç¢ºä¿å³ä½¿å´©æ½°ä¹Ÿæœ‰å ±å‘Š
        try:
            report = {
                'execution_time': datetime.now().isoformat(),
                'has_new': False,
                'status': 'error',
                'error': str(e)
            }
            report_file = Path("data") / "daily_report.json"
            report_file.parent.mkdir(exist_ok=True)
            with open(report_file, 'w') as f:
                json.dump(report, f)
        except:
            pass
        
        exit(1)

if __name__ == "__main__":
    main()
