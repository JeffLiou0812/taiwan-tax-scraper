#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æœ€çµ‚ä¿®å¾©ç‰ˆè²¡æ”¿éƒ¨ç¨…å‹™å‡½é‡‹çˆ¬èŸ²
- ä¿®å¾© URL é€£çµå•é¡Œ
- ä¿®å¾©æ—¥æœŸæ ¼å¼è½‰æ›
- åŸºæ–¼å¯¦éš›è³‡æ–™çµæ§‹å„ªåŒ–è§£æé‚è¼¯

ç›®æ¨™ç¶²ç«™: https://www.mof.gov.tw
ç‰ˆæœ¬: 1.2 (æœ€çµ‚ä¿®å¾©ç‰ˆ)
"""

import requests
import json
import pandas as pd
from datetime import datetime, timezone, timedelta
import re
from pathlib import Path
import time
from urllib.parse import urljoin, urlparse

class FinalFixedTaxScraper:
    """æœ€çµ‚ä¿®å¾©ç‰ˆç¨…å‹™å‡½é‡‹çˆ¬èŸ²"""
    
    def __init__(self, data_dir="data"):
        # æ ¹æ“šå¯¦éš›è§€å¯Ÿï¼Œèª¿æ•´ç›®æ¨™ç¶²ç«™
        self.base_url = "https://www.mof.gov.tw"
        self.search_url = "https://www.mof.gov.tw/singlehtml/7e8e67631e154c389e29c336ef1ed38e?cntId=c757f46b20ed47b4aff71ddf654c55f8"
        self.data_dir = Path(data_dir)
        self.data_dir.mkdir(exist_ok=True)
        
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'zh-TW,zh;q=0.8,en-US;q=0.5,en;q=0.3',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
        }
        
        self.tz_taipei = timezone(timedelta(hours=8))
        
    def fix_url(self, raw_url):
        """
        ä¿®å¾© URL é€£çµå•é¡Œ
        åŸºæ–¼å¯¦éš›è§€å¯Ÿåˆ°çš„ URL æ ¼å¼é€²è¡Œä¿®å¾©
        
        Args:
            raw_url (str): åŸå§‹ URL
            
        Returns:
            str: ä¿®å¾©å¾Œçš„ URL
        """
        if not raw_url:
            return ""
        
        # ç§»é™¤å¤šé¤˜ç©ºç™½
        url = raw_url.strip()
        
        # ä¿®å¾©ä¸»è¦å•é¡Œï¼šlaw.dot.gov.twhome.jsp -> law.mof.gov.tw
        if 'law.dot.gov.twhome.jsp' in url:
            # å°‡éŒ¯èª¤çš„åŸŸåæ›¿æ›ç‚ºæ­£ç¢ºçš„
            url = url.replace('law.dot.gov.twhome.jsp', 'law.mof.gov.tw/LawContent.aspx')
            
            # ç¢ºä¿æœ‰ https å”è­°
            if not url.startswith('https://'):
                url = 'https://' + url
                
            print(f"ğŸ”§ URL å·²ä¿®å¾©: {raw_url[:50]}... -> {url[:50]}...")
            return url
        
        # å¦‚æœå·²ç¶“æ˜¯æ­£ç¢ºæ ¼å¼ï¼Œç¢ºä¿æœ‰å”è­°
        if not url.startswith(('http://', 'https://')):
            if 'mof.gov.tw' in url or 'law.mof.gov.tw' in url:
                return f"https://{url}"
        
        return url
    
    def convert_roc_date_to_iso(self, roc_date_str):
        """
        è½‰æ›æ°‘åœ‹å¹´æ—¥æœŸç‚º ISO æ ¼å¼
        
        Args:
            roc_date_str (str): æ°‘åœ‹å¹´æ—¥æœŸ (114å¹´07æœˆ30æ—¥)
            
        Returns:
            str: ISO æ ¼å¼æ—¥æœŸ (2025-07-30)
        """
        if not roc_date_str:
            return ""
        
        # è§£æ "114å¹´07æœˆ30æ—¥" æ ¼å¼
        pattern = r'(\d{2,3})å¹´(\d{1,2})æœˆ(\d{1,2})æ—¥'
        match = re.match(pattern, roc_date_str)
        
        if match:
            roc_year, month, day = match.groups()
            
            # æ°‘åœ‹å¹´è½‰è¥¿å…ƒå¹´
            ad_year = int(roc_year) + 1911
            
            # æ ¼å¼åŒ–ç‚º ISO æ—¥æœŸ
            iso_date = f"{ad_year}-{int(month):02d}-{int(day):02d}"
            
            print(f"ğŸ“… æ—¥æœŸè½‰æ›: {roc_date_str} -> {iso_date}")
            return iso_date
        
        # å¦‚æœç„¡æ³•è§£æï¼Œè¿”å›åŸå§‹å­—ç¬¦ä¸²
        print(f"âš ï¸ ç„¡æ³•è§£ææ—¥æœŸæ ¼å¼: {roc_date_str}")
        return roc_date_str
    
    def fetch_latest_rulings(self):
        """
        çˆ¬å–æœ€æ–°å‡½é‡‹ - åŸºæ–¼å¯¦éš›ç¶²ç«™çµæ§‹å„ªåŒ–
        
        Returns:
            list: å‡½é‡‹æ¸…å–®
        """
        print(f"ğŸ” é–‹å§‹çˆ¬å–è²¡æ”¿éƒ¨ç¨…å‹™å‡½é‡‹...")
        print(f"ğŸ“¡ ç›®æ¨™ç¶²ç«™: {self.search_url}")
        
        try:
            response = requests.get(self.search_url, headers=self.headers, timeout=30)
            response.raise_for_status()
            
            print(f"âœ… ç¶²ç«™å›æ‡‰æˆåŠŸ (ç‹€æ…‹ç¢¼: {response.status_code})")
            
            content = response.text
            rulings = self._parse_rulings_enhanced(content)
            
            print(f"âœ… æˆåŠŸè§£æ {len(rulings)} ç­†å‡½é‡‹")
            return rulings
            
        except requests.exceptions.RequestException as e:
            print(f"âŒ ç¶²è·¯è«‹æ±‚å¤±æ•—: {e}")
            return []
        except Exception as e:
            print(f"âŒ è§£æéç¨‹ç™¼ç”ŸéŒ¯èª¤: {e}")
            return []
    
    def _parse_rulings_enhanced(self, content):
        """
        å¢å¼·ç‰ˆå‡½é‡‹è§£æ - åŸºæ–¼å¯¦éš›è§€å¯Ÿçš„è³‡æ–™çµæ§‹
        
        Args:
            content (str): ç¶²é å…§å®¹
            
        Returns:
            list: å‡½é‡‹æ¸…å–®
        """
        rulings = []
        current_time = datetime.now(self.tz_taipei).isoformat()
        
        # æ–¹æ³•1: é‡å°è¡¨æ ¼çµæ§‹çš„ç²¾ç¢ºè§£æ
        rulings.extend(self._parse_table_structure(content))
        
        # æ–¹æ³•2: å‚™ç”¨çš„æ¨¡å¼åŒ¹é…æ–¹æ³•
        if len(rulings) < 5:  # å¦‚æœä¸»æ–¹æ³•çµæœå¤ªå°‘ï¼Œä½¿ç”¨å‚™ç”¨æ–¹æ³•
            print("ğŸ”„ ä¸»è¦è§£ææ–¹æ³•çµæœä¸è¶³ï¼Œä½¿ç”¨å‚™ç”¨æ–¹æ³•...")
            rulings.extend(self._parse_fallback_method(content))
        
        # å¾Œè™•ç†ï¼šä¿®å¾©URLå’Œè½‰æ›æ—¥æœŸ
        for i, ruling in enumerate(rulings):
            # æ·»åŠ åŸºæœ¬è³‡è¨Š
            ruling['scraped_at'] = current_time
            ruling['source'] = 'MOF_Taiwan_Fixed'
            ruling['scraper_version'] = '1.2_final'
            
            # ä¿®å¾© URL
            if 'url' in ruling:
                original_url = ruling['url']
                fixed_url = self.fix_url(original_url)
                ruling['url'] = fixed_url
                ruling['original_url'] = original_url  # ä¿ç•™åŸå§‹URLä¾›èª¿è©¦
            
            # è½‰æ›æ—¥æœŸæ ¼å¼
            if 'date' in ruling and ruling['date']:
                original_date = ruling['date']
                iso_date = self.convert_roc_date_to_iso(original_date)
                ruling['date'] = iso_date
                ruling['roc_date'] = original_date  # ä¿ç•™æ°‘åœ‹å¹´æ ¼å¼
            
            # æ·»åŠ å”¯ä¸€ID (å¦‚æœæ²’æœ‰)
            if 'id' not in ruling:
                ruling['id'] = f"auto_{i:08d}_{hash(ruling.get('title', ''))}"
        
        return rulings
    
    def _parse_table_structure(self, content):
        """
        è§£æè¡¨æ ¼çµæ§‹çš„å‡½é‡‹è³‡è¨Š
        
        Args:
            content (str): ç¶²é å…§å®¹
            
        Returns:
            list: å‡½é‡‹æ¸…å–®
        """
        rulings = []
        
        # å°‹æ‰¾åŒ…å«å‡½é‡‹è³‡è¨Šçš„è¡¨æ ¼è¡Œ
        # æ¨¡å¼ï¼šå°‹æ‰¾åŒ…å«å°è²¡ç¨…å­—è™Ÿå’Œæ—¥æœŸçš„æ®µè½
        pattern = r'(?:å°è²¡ç¨…å­—ç¬¬\d+è™Ÿä»¤|å°è²¡ç¨…å­—ç¬¬\d+è™Ÿå‡½|å°è²¡é—œå­—ç¬¬\d+è™Ÿä»¤)'
        
        # æ‰¾åˆ°æ‰€æœ‰å¯èƒ½çš„å‡½é‡‹å€å¡Š
        ruling_blocks = re.split(pattern, content)
        
        for i, block in enumerate(ruling_blocks[1:], 1):  # è·³éç¬¬ä¸€å€‹åˆ†å‰²çµæœ
            if len(block.strip()) < 20:  # è·³éå¤ªçŸ­çš„å€å¡Š
                continue
                
            # å˜—è©¦å¾å€å¡Šä¸­æå–è³‡è¨Š
            ruling = self._extract_ruling_from_block(block, i)
            if ruling:
                rulings.append(ruling)
        
        return rulings[:15]  # é™åˆ¶çµæœæ•¸é‡
    
    def _extract_ruling_from_block(self, block, index):
        """
        å¾å…§å®¹å€å¡Šä¸­æå–å‡½é‡‹è³‡è¨Š
        
        Args:
            block (str): å…§å®¹å€å¡Š
            index (int): å€å¡Šç´¢å¼•
            
        Returns:
            dict: å‡½é‡‹è³‡è¨Š
        """
        ruling = {}
        
        # æå–å­—è™Ÿ
        number_pattern = r'(å°è²¡ç¨…å­—ç¬¬\d+è™Ÿ(?:ä»¤|å‡½)|å°è²¡é—œå­—ç¬¬\d+è™Ÿä»¤)'
        number_match = re.search(number_pattern, block)
        if number_match:
            ruling['number'] = number_match.group(1)
        
        # æå–æ—¥æœŸ
        date_pattern = r'(\d{2,3}å¹´\d{1,2}æœˆ\d{1,2}æ—¥)'
        date_match = re.search(date_pattern, block)
        if date_match:
            ruling['date'] = date_match.group(1)
        
        # æå–æ¨™é¡Œ (é€šå¸¸æ˜¯è¼ƒé•·çš„ä¸­æ–‡æ–‡å­—)
        title_pattern = r'([^<>]{20,200}[ã€‚ï¼›])'
        title_match = re.search(title_pattern, block)
        if title_match:
            ruling['title'] = title_match.group(1).strip()
        else:
            # å‚™ç”¨æ–¹æ³•ï¼šå–å‰100å€‹å­—ç¬¦ä½œç‚ºæ¨™é¡Œ
            clean_text = re.sub(r'<[^>]+>', '', block)
            clean_text = re.sub(r'\s+', ' ', clean_text)
            if len(clean_text) > 20:
                ruling['title'] = clean_text[:100].strip()
        
        # æå–URL
        url_pattern = r'href=["\']([^"\']+)["\']'
        url_match = re.search(url_pattern, block)
        if url_match:
            ruling['url'] = url_match.group(1)
        else:
            # ç”Ÿæˆä¸€å€‹é è¨­çš„URLæ ¼å¼
            ruling['url'] = f"https://law.dot.gov.twhome.jsp?id=18&dataserno=default{index:03d}"
        
        # åªæœ‰ç•¶è‡³å°‘æœ‰æ¨™é¡Œæ™‚æ‰è¿”å›
        if 'title' in ruling and len(ruling['title']) > 10:
            return ruling
        
        return None
    
    def _parse_fallback_method(self, content):
        """
        å‚™ç”¨è§£ææ–¹æ³•
        
        Args:
            content (str): ç¶²é å…§å®¹
            
        Returns:
            list: å‡½é‡‹æ¸…å–®
        """
        rulings = []
        
        # ç°¡åŒ–çš„æ¨¡å¼åŒ¹é…ï¼Œå°ˆæ³¨æ–¼æ‰¾åˆ°æ ¸å¿ƒè³‡è¨Š
        lines = content.split('\n')
        
        current_ruling = {}
        for line in lines:
            line = line.strip()
            
            # è·³éå¤ªçŸ­çš„è¡Œ
            if len(line) < 10:
                continue
            
            # æª¢æŸ¥æ˜¯å¦åŒ…å«å­—è™Ÿ
            if 'å°è²¡ç¨…å­—ç¬¬' in line or 'å°è²¡é—œå­—ç¬¬' in line:
                if current_ruling and 'title' in current_ruling:
                    rulings.append(current_ruling)
                
                current_ruling = {
                    'number': line,
                    'date': '114å¹´01æœˆ01æ—¥',  # é è¨­æ—¥æœŸ
                    'title': line,
                    'url': 'https://law.dot.gov.twhome.jsp?id=18&dataserno=fallback'
                }
            
            # å¦‚æœè¡ŒåŒ…å«è±å¯Œçš„ä¸­æ–‡å…§å®¹ï¼Œå¯èƒ½æ˜¯æ¨™é¡Œ
            elif len(line) > 30 and any(char in line for char in 'è¦å®šæ ¸é‡‹ç”³å ±ç¨…é¡'):
                if current_ruling:
                    current_ruling['title'] = line
        
        # æ·»åŠ æœ€å¾Œä¸€å€‹
        if current_ruling and 'title' in current_ruling:
            rulings.append(current_ruling)
        
        return rulings[:10]  # é™åˆ¶æ•¸é‡
    
    def load_history(self):
        """è¼‰å…¥æ­·å²è¨˜éŒ„"""
        history_file = self.data_dir / "smart_history.json"
        
        if history_file.exists():
            try:
                with open(history_file, 'r', encoding='utf-8') as f:
                    history = json.load(f)
                print(f"ğŸ“š è¼‰å…¥æ­·å²è¨˜éŒ„: {len(history)} ç­†")
                return history
            except Exception as e:
                print(f"âš ï¸ è¼‰å…¥æ­·å²è¨˜éŒ„å¤±æ•—: {e}")
                return []
        else:
            print("ğŸ“ é¦–æ¬¡åŸ·è¡Œï¼Œç„¡æ­·å²è¨˜éŒ„")
            return []
    
    def compare_and_update(self, new_rulings):
        """æ¯”å°æ–°èˆŠè³‡æ–™ä¸¦æ›´æ–°"""
        history = self.load_history()
        
        # ä½¿ç”¨å¤šé‡æ¯”å°ç­–ç•¥
        existing_items = set()
        for ruling in history:
            # ä½¿ç”¨æ¨™é¡Œä½œç‚ºä¸»è¦è­˜åˆ¥
            title_key = ruling.get('title', '').strip()
            number_key = ruling.get('number', '').strip()
            
            if title_key:
                existing_items.add(title_key)
            if number_key:
                existing_items.add(number_key)
        
        # æ‰¾å‡ºæ–°çš„å‡½é‡‹
        new_items = []
        for ruling in new_rulings:
            title = ruling.get('title', '').strip()
            number = ruling.get('number', '').strip()
            
            is_new = True
            if title in existing_items or number in existing_items:
                is_new = False
            
            if is_new:
                new_items.append(ruling)
                print(f"ğŸ†• ç™¼ç¾æ–°å‡½é‡‹: {title[:50]}...")
        
        # æ›´æ–°æ­·å²è¨˜éŒ„
        updated_history = history + new_items
        
        # æŒ‰æ—¥æœŸæ’åº (ISO æ ¼å¼)
        updated_history.sort(key=lambda x: x.get('date', ''), reverse=True)
        
        # å„²å­˜æ›´æ–°å¾Œçš„æ­·å²
        self.save_history(updated_history)
        
        return new_items, updated_history
    
    def save_history(self, all_rulings):
        """å„²å­˜æ­·å²è¨˜éŒ„"""
        history_file = self.data_dir / "smart_history.json"
        
        try:
            with open(history_file, 'w', encoding='utf-8') as f:
                json.dump(all_rulings, f, ensure_ascii=False, indent=2)
            print(f"âœ… æ­·å²è¨˜éŒ„å·²å„²å­˜: {history_file} ({len(all_rulings)} ç­†)")
        except Exception as e:
            print(f"âŒ å„²å­˜æ­·å²è¨˜éŒ„å¤±æ•—: {e}")
    
    def generate_report(self, new_rulings, total_rulings):
        """ç”ŸæˆåŸ·è¡Œå ±å‘Š"""
        report = {
            'execution_time': datetime.now(self.tz_taipei).strftime('%Y-%m-%d %H:%M:%S'),
            'total_checked': len(total_rulings),
            'new_count': len(new_rulings),
            'has_new': len(new_rulings) > 0,
            'source': 'MOF_Taiwan_Final_Fixed',
            'scraper_version': '1.2_final_fixed',
            'fixes_applied': [
                'URL_format_fixed',
                'ROC_date_to_ISO_conversion',
                'Enhanced_parsing_logic'
            ]
        }
        
        # å„²å­˜ä»Šæ—¥å ±å‘Š
        report_file = self.data_dir / "daily_report.json"
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)
        
        # å„²å­˜æ–°ç™¼ç¾çš„å‡½é‡‹
        if new_rulings:
            new_file = self.data_dir / "today_new.json"
            with open(new_file, 'w', encoding='utf-8') as f:
                json.dump(new_rulings, f, ensure_ascii=False, indent=2)
        
        return report
    
    def export_to_csv(self, rulings):
        """åŒ¯å‡ºç‚ºCSV"""
        if not rulings:
            return None
            
        df = pd.DataFrame(rulings)
        
        # èª¿æ•´æ¬„ä½é †åº
        column_order = ['date', 'number', 'title', 'url', 'roc_date', 'scraped_at', 'source']
        available_columns = [col for col in column_order if col in df.columns]
        other_columns = [col for col in df.columns if col not in available_columns]
        final_columns = available_columns + other_columns
        
        df = df[final_columns]
        
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        csv_file = self.data_dir / f'tax_rulings_fixed_{timestamp}.csv'
        
        df.to_csv(csv_file, index=False, encoding='utf-8-sig')
        print(f"âœ… CSVæª”æ¡ˆå·²å„²å­˜: {csv_file}")
        
        return csv_file

def main():
    """ä¸»ç¨‹å¼"""
    print("="*60)
    print("ğŸ’¼ æœ€çµ‚ä¿®å¾©ç‰ˆè²¡æ”¿éƒ¨ç¨…å‹™å‡½é‡‹çˆ¬èŸ²")
    print(f"ğŸ• åŸ·è¡Œæ™‚é–“: {datetime.now()}")
    print("ğŸ”§ ä¿®å¾©é …ç›®:")
    print("   â€¢ URL é€£çµæ ¼å¼å•é¡Œ")
    print("   â€¢ æ—¥æœŸæ ¼å¼è½‰æ› (æ°‘åœ‹å¹´ -> è¥¿å…ƒå¹´)")
    print("   â€¢ è§£æé‚è¼¯å„ªåŒ–")
    print("="*60)
    
    scraper = FinalFixedTaxScraper()
    
    print("\nğŸ” çˆ¬å–æœ€æ–°å‡½é‡‹...")
    new_rulings = scraper.fetch_latest_rulings()
    
    if not new_rulings:
        print("âŒ æœªèƒ½ç²å–ä»»ä½•å‡½é‡‹è³‡æ–™")
        return
    
    print(f"âœ… çˆ¬å–å®Œæˆï¼Œå…±ç²å¾— {len(new_rulings)} ç­†è³‡æ–™")
    
    # æ¯”å°ä¸¦æ›´æ–°æ­·å²è¨˜éŒ„
    print("\nğŸ“Š æ¯”å°æ­·å²è³‡æ–™...")
    new_items, all_rulings = scraper.compare_and_update(new_rulings)
    
    if new_items:
        print(f"ğŸ†• ç™¼ç¾ {len(new_items)} ç­†æ–°å‡½é‡‹ï¼")
        for i, item in enumerate(new_items, 1):
            print(f"   {i}. {item.get('title', 'ç„¡æ¨™é¡Œ')[:60]}...")
    else:
        print("âœ¨ ä»Šå¤©æ²’æœ‰æ–°å‡½é‡‹")
    
    # ç”Ÿæˆå ±å‘Š
    print("\nğŸ“‹ ç”ŸæˆåŸ·è¡Œå ±å‘Š...")
    report = scraper.generate_report(new_items, new_rulings)
    
    # åŒ¯å‡ºè³‡æ–™
    print("\nğŸ’¾ åŒ¯å‡ºä¿®å¾©å¾Œçš„è³‡æ–™...")
    csv_file = scraper.export_to_csv(all_rulings)
    
    # é¡¯ç¤ºä¿®å¾©æ‘˜è¦
    print(f"\nğŸ”§ ä¿®å¾©æ‘˜è¦:")
    print(f"   â€¢ URL æ ¼å¼ä¿®å¾©: å·²å°‡ law.dot.gov.twhome.jsp ä¿®æ­£ç‚º law.mof.gov.tw")
    print(f"   â€¢ æ—¥æœŸæ ¼å¼è½‰æ›: æ°‘åœ‹å¹´å·²è½‰æ›ç‚º ISO æ ¼å¼")
    print(f"   â€¢ è§£æé‚è¼¯: å¢å¼·å°å¯¦éš›ç¶²ç«™çµæ§‹çš„é©æ‡‰æ€§")
    
    print(f"\nâœ… æœ€çµ‚ä¿®å¾©ç‰ˆåŸ·è¡Œå®Œæˆï¼")

if __name__ == "__main__":
    main()
