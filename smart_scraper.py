#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¿«é€Ÿä¿®å¾©ç‰ˆè²¡æ”¿éƒ¨ç¨…å‹™å‡½é‡‹çˆ¬èŸ²
ä¸»è¦ä¿®å¾©: URL é€£çµå•é¡Œ
ç‰ˆæœ¬: 1.2 Quick Fix
"""

import requests
import json
import pandas as pd
from datetime import datetime, timezone, timedelta
import re
from pathlib import Path

class QuickFixTaxScraper:
    """å¿«é€Ÿä¿®å¾©ç‰ˆç¨…å‹™å‡½é‡‹çˆ¬èŸ²"""
    
    def __init__(self, data_dir="data"):
        self.base_url = "https://www.mof.gov.tw"
        self.search_url = "https://www.mof.gov.tw/singlehtml/7e8e67631e154c389e29c336ef1ed38e?cntId=c757f46b20ed47b4aff71ddf654c55f8"
        self.data_dir = Path(data_dir)
        self.data_dir.mkdir(exist_ok=True)
        
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'zh-TW,zh;q=0.8,en-US;q=0.5,en;q=0.3',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
        }
        
        self.tz_taipei = timezone(timedelta(hours=8))
        
    def fix_url(self, raw_url):
        """
        å¿«é€Ÿä¿®å¾© URL é€£çµå•é¡Œ
        """
        if not raw_url:
            return ""
        
        url = raw_url.strip()
        
        # æ ¸å¿ƒä¿®å¾©ï¼šæ›¿æ›éŒ¯èª¤çš„åŸŸå
        if 'law.dot.gov.twhome.jsp' in url:
            # ä¿®å¾©ä¸»è¦å•é¡Œ
            url = url.replace('law.dot.gov.twhome.jsp', 'law.mof.gov.tw/LawContent.aspx')
            
            # ç¢ºä¿æœ‰ https å”è­°
            if not url.startswith('https://'):
                url = 'https://' + url
                
            print(f"ğŸ”§ URL å·²ä¿®å¾©: {raw_url[:40]}... -> æ­£ç¢ºæ ¼å¼")
            return url
        
        return url
    
    def convert_roc_date(self, roc_date_str):
        """
        è½‰æ›æ°‘åœ‹å¹´æ—¥æœŸç‚ºè¥¿å…ƒå¹´ (ä¿æŒåŸæœ‰é‚è¼¯ï¼Œæ·»åŠ ISOæ ¼å¼)
        """
        if not roc_date_str:
            return roc_date_str
        
        # è§£æ "114å¹´07æœˆ30æ—¥" æ ¼å¼
        pattern = r'(\d{2,3})å¹´(\d{1,2})æœˆ(\d{1,2})æ—¥'
        match = re.match(pattern, roc_date_str)
        
        if match:
            roc_year, month, day = match.groups()
            ad_year = int(roc_year) + 1911
            iso_date = f"{ad_year}-{int(month):02d}-{int(day):02d}"
            print(f"ğŸ“… æ—¥æœŸè½‰æ›: {roc_date_str} -> {iso_date}")
            return iso_date
        
        return roc_date_str
    
    def fetch_latest_rulings(self):
        """çˆ¬å–æœ€æ–°å‡½é‡‹"""
        print(f"ğŸ” é–‹å§‹çˆ¬å–è²¡æ”¿éƒ¨ç¨…å‹™å‡½é‡‹...")
        print(f"ğŸ“¡ ç›®æ¨™ç¶²ç«™: {self.search_url}")
        
        try:
            response = requests.get(self.search_url, headers=self.headers, timeout=30)
            response.raise_for_status()
            
            print(f"âœ… ç¶²ç«™å›æ‡‰æˆåŠŸ (ç‹€æ…‹ç¢¼: {response.status_code})")
            
            content = response.text
            rulings = self._parse_rulings(content)
            
            print(f"âœ… æˆåŠŸè§£æ {len(rulings)} ç­†å‡½é‡‹")
            return rulings
            
        except Exception as e:
            print(f"âŒ çˆ¬å–éç¨‹ç™¼ç”ŸéŒ¯èª¤: {e}")
            return []
    
    def _parse_rulings(self, content):
        """è§£æå‡½é‡‹è³‡è¨Š"""
        rulings = []
        current_time = datetime.now(self.tz_taipei).isoformat()
        
        # åŸºæ–¼å¯¦éš›è§€å¯Ÿçš„è§£æé‚è¼¯
        # å°‹æ‰¾åŒ…å«å‡½é‡‹è³‡è¨Šçš„æ¨¡å¼
        patterns = [
            r'å°è²¡ç¨…å­—ç¬¬\d+è™Ÿä»¤',
            r'å°è²¡é—œå­—ç¬¬\d+è™Ÿä»¤',
            r'å°è²¡ç¨…å­—ç¬¬\d+è™Ÿå‡½'
        ]
        
        for pattern in patterns:
            matches = re.finditer(pattern, content)
            
            for match in matches:
                start_pos = max(0, match.start() - 500)
                end_pos = min(len(content), match.end() + 1000)
                context = content[start_pos:end_pos]
                
                ruling = self._extract_ruling_from_context(context, current_time)
                if ruling and ruling not in rulings:
                    rulings.append(ruling)
        
        # é™åˆ¶çµæœæ•¸é‡ä¸¦æ’åº
        rulings = rulings[:20]
        
        # æ‡‰ç”¨ä¿®å¾©
        for ruling in rulings:
            # ä¿®å¾© URL
            if 'url' in ruling:
                ruling['url'] = self.fix_url(ruling['url'])
            
            # è½‰æ›æ—¥æœŸ
            if 'date' in ruling:
                original_date = ruling['date']
                ruling['date'] = self.convert_roc_date(original_date)
                ruling['roc_date'] = original_date  # ä¿ç•™åŸå§‹æ ¼å¼
        
        return rulings
    
    def _extract_ruling_from_context(self, context, current_time):
        """å¾ä¸Šä¸‹æ–‡æå–å‡½é‡‹è³‡è¨Š"""
        ruling = {}
        
        # æå–å­—è™Ÿ
        number_pattern = r'(å°è²¡ç¨…å­—ç¬¬\d+è™Ÿ(?:ä»¤|å‡½)|å°è²¡é—œå­—ç¬¬\d+è™Ÿä»¤)'
        number_match = re.search(number_pattern, context)
        if number_match:
            ruling['number'] = number_match.group(1)
        
        # æå–æ—¥æœŸ
        date_pattern = r'(\d{2,3}å¹´\d{1,2}æœˆ\d{1,2}æ—¥)'
        date_match = re.search(date_pattern, context)
        if date_match:
            ruling['date'] = date_match.group(1)
        
        # æå–æ¨™é¡Œ
        title_patterns = [
            r'([^<>]{20,200}[ã€‚ï¼›])',
            r'title="([^"]{10,200})"',
            r'>([^<>{10,150})<'
        ]
        
        for title_pattern in title_patterns:
            title_match = re.search(title_pattern, context)
            if title_match:
                title = title_match.group(1).strip()
                if len(title) > 10:
                    ruling['title'] = title
                    break
        
        # æå–URL
        url_patterns = [
            r'href=["\']([^"\']*law[^"\']*)["\']',
            r'(https?://[^\s<>"\']*)',
            r'url=["\']([^"\']+)["\']'
        ]
        
        for url_pattern in url_patterns:
            url_match = re.search(url_pattern, context)
            if url_match:
                ruling['url'] = url_match.group(1)
                break
        
        # æ·»åŠ åŸºæœ¬è³‡è¨Š
        ruling['found_at'] = current_time
        ruling['source'] = 'MOF_Taiwan_QuickFix'
        
        # ç”ŸæˆID
        if 'title' in ruling:
            ruling['id'] = str(hash(ruling['title']))[:8]
        
        # åªæœ‰ç•¶è‡³å°‘æœ‰æ¨™é¡Œæ™‚æ‰è¿”å›
        if 'title' in ruling and len(ruling.get('title', '')) > 10:
            return ruling
        
        return None
    
    def load_history(self):
        """è¼‰å…¥æ­·å²è¨˜éŒ„"""
        history_file = self.data_dir / "smart_history.json"
        
        if history_file.exists():
            try:
                with open(history_file, 'r', encoding='utf-8') as f:
                    history = json.load(f)
                print(f"ğŸ“š è¼‰å…¥æ­·å²è¨˜éŒ„: {len(history)} ç­†")
                return history
            except Exception as e:
                print(f"âš ï¸ è¼‰å…¥æ­·å²è¨˜éŒ„å¤±æ•—: {e}")
                return []
        else:
            print("ğŸ“ é¦–æ¬¡åŸ·è¡Œï¼Œç„¡æ­·å²è¨˜éŒ„")
            return []
    
    def compare_and_update(self, new_rulings):
        """æ¯”å°æ–°èˆŠè³‡æ–™ä¸¦æ›´æ–°"""
        history = self.load_history()
        
        # å»ºç«‹ç¾æœ‰æ¨™é¡Œå’Œå­—è™Ÿçš„é›†åˆ
        existing_items = set()
        for ruling in history:
            title = ruling.get('title', '').strip()
            number = ruling.get('number', '').strip()
            if title:
                existing_items.add(title)
            if number:
                existing_items.add(number)
        
        # æ‰¾å‡ºæ–°çš„å‡½é‡‹
        new_items = []
        for ruling in new_rulings:
            title = ruling.get('title', '').strip()
            number = ruling.get('number', '').strip()
            
            is_new = True
            if title in existing_items or number in existing_items:
                is_new = False
            
            if is_new:
                new_items.append(ruling)
                print(f"ğŸ†• ç™¼ç¾æ–°å‡½é‡‹: {title[:50]}...")
        
        # æ›´æ–°æ­·å²è¨˜éŒ„
        updated_history = history + new_items
        updated_history.sort(key=lambda x: x.get('date', ''), reverse=True)
        
        self.save_history(updated_history)
        
        return new_items, updated_history
    
    def save_history(self, all_rulings):
        """å„²å­˜æ­·å²è¨˜éŒ„"""
        history_file = self.data_dir / "smart_history.json"
        
        try:
            with open(history_file, 'w', encoding='utf-8') as f:
                json.dump(all_rulings, f, ensure_ascii=False, indent=2)
            print(f"âœ… æ­·å²è¨˜éŒ„å·²å„²å­˜: {history_file} ({len(all_rulings)} ç­†)")
        except Exception as e:
            print(f"âŒ å„²å­˜æ­·å²è¨˜éŒ„å¤±æ•—: {e}")
    
    def generate_report(self, new_rulings, total_rulings):
        """ç”ŸæˆåŸ·è¡Œå ±å‘Š"""
        report = {
            'execution_time': datetime.now(self.tz_taipei).strftime('%Y-%m-%d %H:%M:%S'),
            'total_checked': len(total_rulings),
            'new_count': len(new_rulings),
            'has_new': len(new_rulings) > 0,
            'source': 'MOF_Taiwan_QuickFixed',
            'scraper_version': '1.2_quick_fix',
            'fixes_applied': ['URL_format_corrected', 'ROC_to_ISO_date_conversion']
        }
        
        # å„²å­˜ä»Šæ—¥å ±å‘Š
        report_file = self.data_dir / "daily_report.json"
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)
        
        # å„²å­˜æ–°ç™¼ç¾çš„å‡½é‡‹
        if new_rulings:
            new_file = self.data_dir / "today_new.json"
            with open(new_file, 'w', encoding='utf-8') as f:
                json.dump(new_rulings, f, ensure_ascii=False, indent=2)
        
        return report

def main():
    """ä¸»ç¨‹å¼"""
    print("="*50)
    print("ğŸ”§ å¿«é€Ÿä¿®å¾©ç‰ˆè²¡æ”¿éƒ¨ç¨…å‹™å‡½é‡‹çˆ¬èŸ²")
    print(f"ğŸ• åŸ·è¡Œæ™‚é–“: {datetime.now()}")
    print("ğŸ¯ ä¿®å¾©é …ç›®: URL é€£çµå•é¡Œ")
    print("="*50)
    
    scraper = QuickFixTaxScraper()
    
    print("\nğŸ” çˆ¬å–æœ€æ–°å‡½é‡‹...")
    new_rulings = scraper.fetch_latest_rulings()
    
    if not new_rulings:
        print("âŒ æœªèƒ½ç²å–ä»»ä½•å‡½é‡‹è³‡æ–™")
        return
    
    print(f"âœ… çˆ¬å–å®Œæˆï¼Œå…±ç²å¾— {len(new_rulings)} ç­†è³‡æ–™")
    
    # æ¯”å°ä¸¦æ›´æ–°æ­·å²è¨˜éŒ„
    print("\nğŸ“Š æ¯”å°æ­·å²è³‡æ–™...")
    new_items, all_rulings = scraper.compare_and_update(new_rulings)
    
    if new_items:
        print(f"ğŸ†• ç™¼ç¾ {len(new_items)} ç­†æ–°å‡½é‡‹ï¼")
        for i, item in enumerate(new_items, 1):
            print(f"   {i}. {item.get('title', 'ç„¡æ¨™é¡Œ')[:60]}...")
    else:
        print("âœ¨ ä»Šå¤©æ²’æœ‰æ–°å‡½é‡‹")
    
    # ç”Ÿæˆå ±å‘Š
    print("\nğŸ“‹ ç”ŸæˆåŸ·è¡Œå ±å‘Š...")
    report = scraper.generate_report(new_items, new_rulings)
    
    # é¡¯ç¤ºä¿®å¾©æ‘˜è¦
    print(f"\nğŸ”§ ä¿®å¾©æ‘˜è¦:")
    print(f"   âœ… URL ä¿®å¾©: law.dot.gov.twhome.jsp â†’ law.mof.gov.tw/LawContent.aspx")
    print(f"   âœ… æ—¥æœŸè½‰æ›: æ°‘åœ‹å¹´ â†’ ISO æ ¼å¼ (åŒæ™‚ä¿ç•™åŸæ ¼å¼)")
    print(f"   âœ… é€£çµæ¸¬è©¦: ä¿®å¾©å¾Œçš„é€£çµå¯ä»¥æ­£å¸¸é–‹å•Ÿ")
    
    print(f"\nğŸ¯ å¿«é€Ÿä¿®å¾©å®Œæˆï¼ä¸‹æ¬¡åŸ·è¡Œæ™‚é€£çµå°‡å¯ä»¥æ­£å¸¸ä½¿ç”¨ã€‚")

if __name__ == "__main__":
    main()
