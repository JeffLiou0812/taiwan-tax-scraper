#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
è²¡æ”¿éƒ¨è³¦ç¨…ç½²æ³•è¦æŸ¥è©¢ç³»çµ± - æ–°é ’å‡½é‡‹çˆ¬èŸ²ï¼ˆå®Œæ•´ä¿®æ­£ç‰ˆï¼‰
åŒ…å«æ¨™é¡Œ/ä¸»æ—¨æå–æ”¹é€²

ç‰ˆæœ¬: 7.0 Complete Fixed
æ›´æ–°æ—¥æœŸ: 2025-08-20
ç›®æ¨™ç¶²ç«™: https://law.dot.gov.tw/law-ch/home.jsp

ä¿®æ­£å…§å®¹ï¼š
âœ… æ­£ç¢ºæå–å‡½é‡‹ä¸»æ—¨ä½œç‚ºæ¨™é¡Œ
âœ… å®Œæ•´éŒ¯èª¤é˜²è­·æ©Ÿåˆ¶
âœ… ä¿ç•™åŸå§‹æ—¥æœŸæ ¼å¼ï¼ˆä¸è½‰æ›ï¼‰
âœ… URLä¿®å¾©æ©Ÿåˆ¶
"""

import requests
import json
import pandas as pd
from datetime import datetime, timezone, timedelta
import re
from pathlib import Path
import hashlib
import time
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse, urlencode
import logging
from typing import Dict, List, Tuple, Optional
import traceback

class TaxRulingScraper:
    """è²¡æ”¿éƒ¨è³¦ç¨…ç½²å‡½é‡‹çˆ¬èŸ² - å®Œæ•´ä¿®æ­£ç‰ˆ"""
    
    def __init__(self, data_dir="data", debug=True):
        """åˆå§‹åŒ–çˆ¬èŸ²"""
        # ä½¿ç”¨Pathç‰©ä»¶è™•ç†è·¯å¾‘
        self.data_dir = Path(data_dir)
        self.data_dir.mkdir(exist_ok=True)
        
        # è¨­å®šæ—¥èªŒç³»çµ±
        self.setup_logging(debug)
        
        # ç¶²ç«™è¨­å®š
        self.base_url = "https://law.dot.gov.tw"
        self.search_url = "https://law.dot.gov.tw/law-ch/home.jsp"
        
        # æŸ¥è©¢åƒæ•¸
        self.search_params = {
            'id': '18',
            'contentid': '18',
            'parentpath': '0,7',
            'mcustomize': 'newlaw_list.jsp',
            'istype': 'L',
            'classtablename': 'LawClass',
            'sort': '1',
            'up_down': 'D'
        }
        
        # å°ç£æ™‚å€
        self.tz_taipei = timezone(timedelta(hours=8))
        
        # å®Œæ•´çš„è«‹æ±‚æ¨™é ­
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8',
            'Accept-Language': 'zh-TW,zh;q=0.9,en;q=0.8,zh-CN;q=0.7',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Cache-Control': 'max-age=0',
            'Upgrade-Insecure-Requests': '1',
            'Referer': 'https://law.dot.gov.tw/',
            'DNT': '1'
        }
        
        # å»ºç«‹Session
        self.session = requests.Session()
        self.session.headers.update(self.headers)
        
        # éŒ¯èª¤çµ±è¨ˆ
        self.error_stats = {
            'url_errors_fixed': 0,
            'parse_errors_recovered': 0,
            'retry_attempts': 0,
            'total_errors': 0
        }
    
    def setup_logging(self, debug: bool):
        """è¨­å®šæ—¥èªŒç³»çµ±"""
        log_level = logging.DEBUG if debug else logging.INFO
        logging.basicConfig(
            level=log_level,
            format='%(asctime)s - %(levelname)s - %(message)s',
            datefmt='%Y-%m-%d %H:%M:%S'
        )
        self.logger = logging.getLogger(__name__)
    
    def safe_request(self, url: str, params: Dict = None, max_retries: int = 3) -> Optional[requests.Response]:
        """å®‰å…¨çš„ç¶²è·¯è«‹æ±‚ï¼ŒåŒ…å«é‡è©¦æ©Ÿåˆ¶"""
        for attempt in range(max_retries):
            try:
                self.logger.debug(f"è«‹æ±‚å˜—è©¦ {attempt + 1}/{max_retries}: {url}")
                
                response = self.session.get(
                    url,
                    params=params,
                    timeout=30,
                    verify=True,
                    allow_redirects=True
                )
                
                if response.status_code == 200:
                    self.logger.debug("è«‹æ±‚æˆåŠŸ")
                    return response
                elif response.status_code == 404:
                    self.logger.error(f"é é¢ä¸å­˜åœ¨ (404): {url}")
                    return None
                else:
                    self.logger.warning(f"HTTP {response.status_code}")
                    self.error_stats['retry_attempts'] += 1
                    
            except requests.RequestException as e:
                self.logger.error(f"è«‹æ±‚éŒ¯èª¤ (å˜—è©¦ {attempt + 1}/{max_retries}): {e}")
                self.error_stats['total_errors'] += 1
            
            if attempt < max_retries - 1:
                wait_time = min(2 ** attempt, 10)
                self.logger.info(f"ç­‰å¾… {wait_time} ç§’å¾Œé‡è©¦...")
                time.sleep(wait_time)
        
        self.logger.error(f"æ‰€æœ‰é‡è©¦éƒ½å¤±æ•—: {url}")
        return None
    
    def fix_url_comprehensive(self, url: str) -> str:
        """å…¨é¢çš„URLä¿®å¾©æ©Ÿåˆ¶"""
        if not url:
            return ""
        
        original_url = url
        url = str(url).strip()
        
        # æª¢æ¸¬ä¸¦ä¿®å¾©å·²çŸ¥çš„éŒ¯èª¤æ¨¡å¼
        error_patterns = [
            ('twhome.jsp', '/home.jsp'),
            ('gov.twhome', 'gov.tw/home'),
            ('lawlaw', 'law'),
            ('//', '/')
        ]
        
        for error_pattern, correct_pattern in error_patterns:
            if error_pattern in url and error_pattern != '//':
                self.logger.warning(f"åµæ¸¬åˆ°URLéŒ¯èª¤æ¨¡å¼: {error_pattern}")
                url = url.replace(error_pattern, correct_pattern)
                self.error_stats['url_errors_fixed'] += 1
        
        # è™•ç†é›™æ–œç·š
        if '//' in url and not url.startswith('http'):
            url = re.sub(r'(?<!:)//', '/', url)
        
        # ç¢ºä¿URLå®Œæ•´æ€§
        if not url.startswith(('http://', 'https://')):
            if url.startswith('/'):
                url = self.base_url + url
            else:
                url = urljoin(f"{self.base_url}/law-ch/", url)
        
        # å¼·åˆ¶HTTPS
        if url.startswith('http://law.dot.gov.tw'):
            url = url.replace('http://', 'https://')
        
        # æœ€çµ‚é©—è­‰
        try:
            result = urlparse(url)
            if not all([result.scheme, result.netloc]):
                self.logger.error(f"URLé©—è­‰å¤±æ•—: {url}")
                return original_url
        except:
            return original_url
        
        if url != original_url:
            self.logger.info(f"URLå·²ä¿®å¾©: {original_url} -> {url}")
        
        return url
    
    def extract_date(self, text: str) -> str:
        """æå–æ—¥æœŸä½†ä¸è½‰æ›ï¼ˆä¿ç•™åŸå§‹æ°‘åœ‹å¹´æ ¼å¼ï¼‰"""
        if not text:
            return ""
        
        # æœå°‹å„ç¨®æ—¥æœŸæ ¼å¼
        date_patterns = [
            r'\d{2,3}å¹´\d{1,2}æœˆ\d{1,2}æ—¥',
            r'\d{2,3}\.\d{1,2}\.\d{1,2}',
            r'\d{2,3}/\d{1,2}/\d{1,2}',
            r'æ°‘åœ‹\d{2,3}å¹´\d{1,2}æœˆ\d{1,2}æ—¥',
        ]
        
        for pattern in date_patterns:
            match = re.search(pattern, text)
            if match:
                return match.group(0)
        
        return ""
    
    def parse_rulings_smart(self, html_content: str) -> List[Dict]:
        """æ™ºèƒ½è§£æï¼Œä½¿ç”¨å¤šé‡ç­–ç•¥"""
        rulings = []
        
        try:
            soup = BeautifulSoup(html_content, 'html.parser')
            
            # ç­–ç•¥1ï¼šè¡¨æ ¼è§£æ
            self.logger.debug("å˜—è©¦è¡¨æ ¼è§£æç­–ç•¥...")
            tables = soup.find_all('table')
            
            for table in tables:
                if 'navigation' in str(table.get('class', [])).lower():
                    continue
                
                rows = table.find_all('tr')
                for row in rows[1:]:  # è·³éæ¨™é¡Œåˆ—
                    cells = row.find_all(['td', 'th'])
                    if len(cells) >= 2:
                        ruling = self.extract_ruling_from_cells(cells)
                        if ruling:
                            rulings.append(ruling)
            
            # ç­–ç•¥2ï¼šåˆ—è¡¨è§£æ
            if not rulings:
                self.logger.debug("è¡¨æ ¼è§£æç„¡çµæœï¼Œå˜—è©¦åˆ—è¡¨è§£æ...")
                containers = soup.find_all(['div', 'ul', 'ol'], class_=re.compile(r'law|list|item|content'))
                
                for container in containers:
                    items = container.find_all(['li', 'div', 'p'])
                    for item in items:
                        ruling = self.extract_ruling_from_element(item)
                        if ruling:
                            rulings.append(ruling)
            
            # ç­–ç•¥3ï¼šå…¨æ–‡æœå°‹
            if not rulings:
                self.logger.debug("åˆ—è¡¨è§£æç„¡çµæœï¼Œä½¿ç”¨å…¨æ–‡æœå°‹...")
                all_text = soup.get_text()
                lines = all_text.split('\n')
                
                for i, line in enumerate(lines):
                    if self.extract_date(line):
                        ruling = {
                            'date': self.extract_date(line),
                            'title': lines[i+1] if i+1 < len(lines) else line,
                            'source': 'DOT_Taiwan',
                            'scrape_time': datetime.now(self.tz_taipei).isoformat()
                        }
                        ruling['id'] = self.generate_id(ruling)
                        rulings.append(ruling)
            
            self.logger.info(f"æˆåŠŸè§£æ {len(rulings)} ç­†å‡½é‡‹")
            
        except Exception as e:
            self.logger.error(f"è§£æéŒ¯èª¤: {e}")
            self.error_stats['total_errors'] += 1
            self.error_stats['parse_errors_recovered'] += 1
        
        return rulings
    
    def extract_ruling_from_cells(self, cells) -> Optional[Dict]:
        """
        å¾è¡¨æ ¼å„²å­˜æ ¼æå–å‡½é‡‹è³‡è¨Š
        æ”¹é€²ç‰ˆï¼šç¢ºä¿æ­£ç¢ºæå–ä¸»æ—¨ä½œç‚ºæ¨™é¡Œ
        """
        try:
            ruling = {
                'source': 'DOT_Taiwan',
                'scrape_time': datetime.now(self.tz_taipei).isoformat()
            }
            
            has_content = False
            doc_number = ""
            title_content = ""
            date_text = ""
            
            # ç¬¬ä¸€è¼ªï¼šæ”¶é›†æ‰€æœ‰è³‡è¨Š
            for i, cell in enumerate(cells):
                cell_text = cell.get_text(strip=True)
                
                # æå–æ—¥æœŸ
                if not date_text:
                    date = self.extract_date(cell_text)
                    if date:
                        date_text = date
                        ruling['date'] = date
                        has_content = True
                
                # æå–å­—è™Ÿ
                if re.search(r'[å°è²¡ç¨…].*?ç¬¬?\d+è™Ÿ', cell_text):
                    doc_number = cell_text
                    ruling['doc_number'] = doc_number
                    has_content = True
                
                # æå–é€£çµ
                links = cell.find_all('a')
                for link in links:
                    link_text = link.get_text(strip=True)
                    href = link.get('href', '')
                    
                    # åˆ¤æ–·é€£çµæ–‡å­—æ˜¯å¦ç‚ºä¸»æ—¨
                    if link_text and not re.match(r'^[å°è²¡ç¨…].*?ç¬¬?\d+è™Ÿ', link_text):
                        # é€™å€‹é€£çµæ–‡å­—ä¸æ˜¯å­—è™Ÿï¼Œæ‡‰è©²æ˜¯ä¸»æ—¨
                        if len(link_text) > len(title_content):
                            title_content = link_text
                    
                    if href:
                        ruling['url'] = self.fix_url_comprehensive(href)
                        ruling['original_url'] = href
                        has_content = True
                
                # å¦‚æœé€™å€‹å„²å­˜æ ¼æœ‰è¼ƒé•·çš„æ–‡å­—ï¼Œä¸”ä¸æ˜¯æ—¥æœŸæˆ–å­—è™Ÿï¼Œå¯èƒ½æ˜¯ä¸»æ—¨
                if len(cell_text) > 20:
                    # æ’é™¤ç´”æ—¥æœŸã€å­—è™Ÿ
                    if not re.match(r'^[\då¹´æœˆæ—¥\.\/ ]+$', cell_text) and \
                       not re.search(r'^[å°è²¡ç¨…].*?ç¬¬?\d+è™Ÿ$', cell_text):
                        # å¦‚æœé‚„æ²’æœ‰æ¨™é¡Œï¼Œæˆ–é€™å€‹æ–‡å­—æ›´é•·æ›´è©³ç´°
                        if not title_content or len(cell_text) > len(title_content):
                            title_content = cell_text[:500]  # é™åˆ¶é•·åº¦
            
            # ç¬¬äºŒè¼ªï¼šçµ„åˆæ¨™é¡Œ
            if title_content:
                ruling['title'] = title_content
            elif doc_number:
                # å¦‚æœçœŸçš„æ²’æœ‰æ‰¾åˆ°ä¸»æ—¨ï¼Œè‡³å°‘ç”¨å­—è™Ÿ
                ruling['title'] = doc_number
            else:
                # æœ€å¾Œçš„å‚™æ¡ˆ
                ruling['title'] = date_text if date_text else "ç„¡æ¨™é¡Œ"
            
            # ç¢ºä¿æœ‰åŸºæœ¬è³‡è¨Šæ‰è¿”å›
            if has_content and (ruling.get('date') or ruling.get('doc_number') or len(ruling.get('title', '')) > 10):
                ruling['id'] = self.generate_id(ruling)
                self.logger.debug(f"æå–æˆåŠŸ - æ¨™é¡Œ: {ruling.get('title', '')[:50]}...")
                return ruling
                
        except Exception as e:
            self.logger.debug(f"å„²å­˜æ ¼æå–éŒ¯èª¤: {e}")
            
        return None
    
    def extract_ruling_from_element(self, element) -> Optional[Dict]:
        """å¾HTMLå…ƒç´ æå–å‡½é‡‹è³‡è¨Š"""
        try:
            text = element.get_text(strip=True)
            
            if len(text) < 10:
                return None
            
            ruling = {
                'source': 'DOT_Taiwan',
                'scrape_time': datetime.now(self.tz_taipei).isoformat()
            }
            
            # æå–æ—¥æœŸ
            date = self.extract_date(text)
            if date:
                ruling['date'] = date
            
            # æå–å­—è™Ÿ
            doc_match = re.search(r'([å°è²¡ç¨…].*?ç¬¬?\d+è™Ÿ)', text)
            if doc_match:
                ruling['doc_number'] = doc_match.group(1)
            
            # æå–é€£çµå’Œä¸»æ—¨
            links = element.find_all('a')
            title_content = ""
            
            for link in links:
                link_text = link.get_text(strip=True)
                href = link.get('href', '')
                
                # å„ªå…ˆä½¿ç”¨éå­—è™Ÿçš„é€£çµæ–‡å­—ä½œç‚ºæ¨™é¡Œ
                if link_text and not re.match(r'^[å°è²¡ç¨…].*?ç¬¬?\d+è™Ÿ', link_text):
                    if len(link_text) > len(title_content):
                        title_content = link_text
                
                if href:
                    ruling['url'] = self.fix_url_comprehensive(href)
            
            # è¨­å®šæ¨™é¡Œ
            if title_content:
                ruling['title'] = title_content
            elif doc_match:
                # å°‹æ‰¾å­—è™Ÿä¹‹å¾Œçš„æ–‡å­—ä½œç‚ºä¸»æ—¨
                after_doc = text[doc_match.end():].strip()
                if after_doc and len(after_doc) > 10:
                    ruling['title'] = after_doc[:300]
                else:
                    ruling['title'] = text[:200]
            else:
                ruling['title'] = text[:200]
            
            # åªæœ‰åœ¨æœ‰å¯¦è³ªå…§å®¹æ™‚æ‰è¿”å›
            if ruling.get('date') or ruling.get('title'):
                ruling['id'] = self.generate_id(ruling)
                return ruling
                
        except Exception as e:
            self.logger.debug(f"å…ƒç´ æå–éŒ¯èª¤: {e}")
            
        return None
    
    def generate_id(self, ruling: Dict) -> str:
        """ç”Ÿæˆå”¯ä¸€è­˜åˆ¥ç¢¼"""
        key_parts = [
            ruling.get('date', ''),
            ruling.get('doc_number', ''),
            ruling.get('title', '')[:50]
        ]
        key_string = '|'.join(filter(None, key_parts))
        return hashlib.md5(key_string.encode('utf-8')).hexdigest()[:12]
    
    def fetch_new_rulings(self, max_pages: int = 3) -> List[Dict]:
        """ä¸»è¦çˆ¬å–å‡½æ•¸"""
        all_rulings = []
        
        self.logger.info("="*60)
        self.logger.info("é–‹å§‹çˆ¬å–è²¡æ”¿éƒ¨è³¦ç¨…ç½²æ–°é ’å‡½é‡‹")
        self.logger.info(f"ç›®æ¨™: {self.search_url}")
        self.logger.info("="*60)
        
        for page in range(1, max_pages + 1):
            try:
                params = self.search_params.copy()
                if page > 1:
                    params['page'] = str(page)
                
                self.logger.info(f"\næ­£åœ¨çˆ¬å–ç¬¬ {page} é ...")
                
                response = self.safe_request(self.search_url, params)
                
                if not response:
                    self.logger.warning(f"ç¬¬ {page} é ç„¡æ³•å–å¾—")
                    if page == 1:
                        self.logger.error("ç„¡æ³•å–å¾—ç¬¬ä¸€é è³‡æ–™ï¼Œåœæ­¢çˆ¬å–")
                        break
                    continue
                
                page_rulings = self.parse_rulings_smart(response.text)
                
                if not page_rulings:
                    self.logger.info(f"ç¬¬ {page} é ç„¡è³‡æ–™ï¼Œåœæ­¢çˆ¬å–")
                    break
                
                all_rulings.extend(page_rulings)
                self.logger.info(f"ç¬¬ {page} é æˆåŠŸ: {len(page_rulings)} ç­†")
                
                # é¡¯ç¤ºæå–çš„æ¨™é¡Œï¼ˆç”¨æ–¼é©—è­‰ï¼‰
                for ruling in page_rulings[:2]:  # é¡¯ç¤ºå‰2ç­†
                    self.logger.debug(f"  æ¨™é¡Œ: {ruling.get('title', 'N/A')[:50]}...")
                
                if page < max_pages:
                    time.sleep(1)
                    
            except Exception as e:
                self.logger.error(f"çˆ¬å–ç¬¬ {page} é ç™¼ç”ŸéŒ¯èª¤: {e}")
                self.error_stats['total_errors'] += 1
                continue
        
        self.logger.info(f"\nç¸½å…±çˆ¬å–: {len(all_rulings)} ç­†å‡½é‡‹")
        return all_rulings
    
    def compare_and_update(self, new_rulings: List[Dict]) -> Tuple[List[Dict], List[Dict]]:
        """æ¯”å°æ­·å²è¨˜éŒ„ï¼Œæ‰¾å‡ºæ–°å¢çš„å‡½é‡‹"""
        history_file = self.data_dir / "smart_history.json"
        
        history = []
        if history_file.exists():
            try:
                with open(history_file, 'r', encoding='utf-8') as f:
                    content = f.read()
                    if content:
                        history = json.loads(content)
                    self.logger.info(f"è¼‰å…¥ {len(history)} ç­†æ­·å²è¨˜éŒ„")
            except json.JSONDecodeError as e:
                self.logger.error(f"JSONè§£æéŒ¯èª¤: {e}")
                backup_file = history_file.with_suffix('.json.backup')
                history_file.rename(backup_file)
                self.logger.info(f"å·²å‚™ä»½æå£çš„æ­·å²æª”æ¡ˆåˆ° {backup_file}")
            except Exception as e:
                self.logger.error(f"è®€å–æ­·å²è¨˜éŒ„å¤±æ•—: {e}")
        
        # æ¯”å°æ–°è³‡æ–™
        history_ids = {item.get('id') for item in history if item.get('id')}
        new_items = []
        
        for ruling in new_rulings:
            if ruling.get('id') and ruling['id'] not in history_ids:
                new_items.append(ruling)
        
        self.logger.info(f"ç™¼ç¾ {len(new_items)} ç­†æ–°å‡½é‡‹")
        
        # æ›´æ–°æ­·å²è¨˜éŒ„
        if new_items:
            history.extend(new_items)
            
            if len(history) > 1000:
                history = history[-1000:]
            
            try:
                temp_file = history_file.with_suffix('.json.tmp')
                with open(temp_file, 'w', encoding='utf-8') as f:
                    json.dump(history, f, ensure_ascii=False, indent=2)
                
                with open(temp_file, 'r', encoding='utf-8') as f:
                    json.load(f)
                
                temp_file.replace(history_file)
                self.logger.info("æ­·å²è¨˜éŒ„å·²å®‰å…¨æ›´æ–°")
                
            except Exception as e:
                self.logger.error(f"æ›´æ–°æ­·å²è¨˜éŒ„å¤±æ•—: {e}")
                if temp_file.exists():
                    temp_file.unlink()
        
        return new_items, history
    
    def generate_report(self, new_items: List[Dict], total_rulings: List[Dict]) -> Dict:
        """ç”ŸæˆåŸ·è¡Œå ±å‘Š"""
        report = {
            'execution_time': datetime.now(self.tz_taipei).isoformat(),
            'execution_date': datetime.now(self.tz_taipei).strftime('%Y-%m-%d'),
            'total_checked': len(total_rulings),
            'new_count': len(new_items),
            'has_new': len(new_items) > 0,
            'source': 'law.dot.gov.tw',
            'scraper_version': '7.0_Complete_Fixed',
            'error_statistics': self.error_stats,
            'status': 'success' if total_rulings else 'no_data'
        }
        
        # å„²å­˜å ±å‘Š
        report_file = self.data_dir / "daily_report.json"
        try:
            with open(report_file, 'w', encoding='utf-8') as f:
                json.dump(report, f, ensure_ascii=False, indent=2)
            self.logger.info("å ±å‘Šå·²ç”Ÿæˆ")
        except Exception as e:
            self.logger.error(f"å ±å‘Šç”Ÿæˆå¤±æ•—: {e}")
            basic_report = {'has_new': False, 'status': 'error'}
            with open(report_file, 'w') as f:
                json.dump(basic_report, f)
        
        # å„²å­˜æ–°å‡½é‡‹
        if new_items:
            new_file = self.data_dir / "today_new.json"
            try:
                with open(new_file, 'w', encoding='utf-8') as f:
                    json.dump(new_items, f, ensure_ascii=False, indent=2)
                self.logger.info(f"æ–°å‡½é‡‹å·²å„²å­˜: {len(new_items)} ç­†")
            except Exception as e:
                self.logger.error(f"å„²å­˜æ–°å‡½é‡‹å¤±æ•—: {e}")
        
        return report
    
    def save_results(self, rulings: List[Dict]) -> None:
        """å„²å­˜çµæœç‚ºå¤šç¨®æ ¼å¼"""
        if not rulings:
            return
        
        timestamp = datetime.now(self.tz_taipei).strftime('%Y%m%d_%H%M%S')
        
        # JSONæ ¼å¼
        json_file = self.data_dir / f'smart_results_{timestamp}.json'
        try:
            with open(json_file, 'w', encoding='utf-8') as f:
                json.dump(rulings, f, ensure_ascii=False, indent=2)
            self.logger.info(f"JSONå·²å„²å­˜: {json_file.name}")
        except Exception as e:
            self.logger.error(f"JSONå„²å­˜å¤±æ•—: {e}")
        
        # CSVæ ¼å¼
        csv_file = self.data_dir / f'smart_results_{timestamp}.csv'
        try:
            df = pd.DataFrame(rulings)
            df.to_csv(csv_file, index=False, encoding='utf-8-sig')
            self.logger.info(f"CSVå·²å„²å­˜: {csv_file.name}")
        except Exception as e:
            self.logger.error(f"CSVå„²å­˜å¤±æ•—: {e}")

def main():
    """ä¸»ç¨‹å¼"""
    print("="*70)
    print("ğŸ›ï¸ è²¡æ”¿éƒ¨è³¦ç¨…ç½²æ–°é ’å‡½é‡‹çˆ¬èŸ² - å®Œæ•´ä¿®æ­£ç‰ˆ")
    print(f"ğŸ“ ç›®æ¨™ç¶²ç«™: law.dot.gov.tw")
    print(f"ğŸ• åŸ·è¡Œæ™‚é–“: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"ğŸ“Œ ç‰ˆæœ¬: 7.0 - åŒ…å«æ¨™é¡Œæå–æ”¹é€²")
    print("="*70)
    
    try:
        # åˆå§‹åŒ–
        print("\nâš™ï¸ åˆå§‹åŒ–ç³»çµ±...")
        scraper = TaxRulingScraper(debug=True)
        
        # çˆ¬å–
        print("\nğŸ“¡ é–‹å§‹çˆ¬å–...")
        rulings = scraper.fetch_new_rulings(max_pages=3)
        
        if not rulings:
            print("\nâš ï¸ æœªçˆ¬å–åˆ°è³‡æ–™")
            scraper.generate_report([], [])
            return
        
        print(f"\nâœ… æˆåŠŸçˆ¬å– {len(rulings)} ç­†å‡½é‡‹")
        
        # è³‡æ–™é è¦½ï¼ˆé¡¯ç¤ºæ¨™é¡Œé©—è­‰ï¼‰
        print("\nğŸ“‹ è³‡æ–™é è¦½ï¼ˆé©—è­‰æ¨™é¡Œæå–ï¼‰ï¼š")
        for i, ruling in enumerate(rulings[:5], 1):
            print(f"\n  {i}. æ¨™é¡Œ: {ruling.get('title', 'N/A')[:80]}")
            print(f"     æ—¥æœŸ: {ruling.get('date', 'N/A')}")
            print(f"     å­—è™Ÿ: {ruling.get('doc_number', 'N/A')}")
        
        # æ¯”å°æ­·å²
        print("\nğŸ” æ¯”å°æ­·å²è¨˜éŒ„...")
        new_items, history = scraper.compare_and_update(rulings)
        
        if new_items:
            print(f"\nğŸ‰ ç™¼ç¾ {len(new_items)} ç­†æ–°å‡½é‡‹")
            for i, item in enumerate(new_items[:3], 1):
                print(f"\n  {i}. æ¨™é¡Œ: {item.get('title', 'N/A')[:80]}")
                print(f"     æ—¥æœŸ: {item.get('date', 'N/A')}")
        else:
            print("\nâœ¨ ç„¡æ–°å‡½é‡‹")
        
        # å„²å­˜
        print("\nğŸ’¾ å„²å­˜è³‡æ–™...")
        scraper.save_results(rulings)
        
        # å ±å‘Š
        print("\nğŸ“Š ç”Ÿæˆå ±å‘Š...")
        report = scraper.generate_report(new_items, rulings)
        
        # çµ±è¨ˆ
        print("\nğŸ“ˆ åŸ·è¡Œçµ±è¨ˆï¼š")
        print(f"  â€¢ ç¸½è™•ç†æ•¸: {len(rulings)}")
        print(f"  â€¢ æ–°å‡½é‡‹æ•¸: {len(new_items)}")
        print(f"  â€¢ URLä¿®å¾©: {report['error_statistics']['url_errors_fixed']}")
        print(f"  â€¢ éŒ¯èª¤æ¢å¾©: {report['error_statistics']['parse_errors_recovered']}")
        
        print("\nâœ… åŸ·è¡Œå®Œæˆï¼")
        
    except KeyboardInterrupt:
        print("\nâš ï¸ ä½¿ç”¨è€…ä¸­æ–·åŸ·è¡Œ")
    except Exception as e:
        print(f"\nâŒ åš´é‡éŒ¯èª¤: {e}")
        traceback.print_exc()
        
        # ç¢ºä¿æœ‰å ±å‘Š
        try:
            report = {
                'execution_time': datetime.now().isoformat(),
                'has_new': False,
                'status': 'error',
                'error': str(e)
            }
            report_file = Path("data") / "daily_report.json"
            report_file.parent.mkdir(exist_ok=True)
            with open(report_file, 'w') as f:
                json.dump(report, f)
        except:
            pass
        
        exit(1)

if __name__ == "__main__":
    main()
